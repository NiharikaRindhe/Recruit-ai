#ganpatibappamorya
import os
import re
import json
# OAuth & HTTP
import base64, secrets, hashlib
from urllib.parse import urlencode
import httpx
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from typing import Optional, List, Dict, Any
from fastapi import FastAPI, Depends, HTTPException, status, UploadFile, File, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import JSONResponse
from pydantic import BaseModel, EmailStr, Field
from supabase import create_client, Client
from dotenv import load_dotenv
import jwt
import requests
# --- MeetAI extra imports (add to existing imports) ---
import asyncio
import time
import threading
import queue
from pathlib import Path
from functools import lru_cache

from fastapi import WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse  # you already import JSONResponse

from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi

# Google Cloud STT
from google.cloud import speech
from google.oauth2 import service_account

# imports
from supabase.client import ClientOptions
import datetime
from fastapi import Request
from fastapi.responses import RedirectResponse
from uuid import UUID
import fitz  # PyMuPDF
# Safe import for different storage3 versions
try:
    from storage3.utils import UploadFileOptions  # v2+
except Exception:  # older SDKs won't have it
    UploadFileOptions = None

# secure hashing (recommended)
# -------- Password hashing (Passlib preferred; safe fallback) --------
try:
    from passlib.context import CryptContext  # pip install passlib[bcrypt]
    pwd_context = CryptContext(schemes=["bcrypt_sha256"], deprecated="auto")

    def _hash_password(p: str) -> str:
        return pwd_context.hash(p)

    def _verify_password(p: str, h: str) -> bool:
        return pwd_context.verify(p, h)

except Exception:
    # Fallback: salted sha256 (only if passlib isn't installed) – less secure
    import os, base64, hashlib

    def _hash_password(p: str) -> str:
        salt = os.urandom(16)
        h = hashlib.sha256(salt + p.encode("utf-8")).digest()
        return "sha256$" + base64.b64encode(salt + h).decode("utf-8")

    def _verify_password(p: str, h: str) -> bool:
        if not h.startswith("sha256$"):
            return False
        raw = base64.b64decode(h.split("sha256$", 1)[1].encode("utf-8"))
        salt, digest = raw[:16], raw[16:]
        return hashlib.sha256(salt + p.encode("utf-8")).digest() == digest

import datetime
from fastapi import Request
from fastapi.responses import RedirectResponse
import fitz  # PyMuPDF

# Add this block:
try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except Exception:
    ZoneInfo = None

# ---------------------------------------------------------------------------
# env
# ---------------------------------------------------------------------------
load_dotenv()

ENV = os.getenv("ENV", "local").lower()

if ENV in ("local", "dev", "development"):
    # local HTTP testing
    COOKIE_SAMESITE = "Lax"    # cookies still sent for top-level GETs
    COOKIE_SECURE = False      # allow http://localhost
else:
    # Render / production (must be HTTPS)
    COOKIE_SAMESITE = "None"   # allow cross-site redirects
    COOKIE_SECURE = True       # required when SameSite=None


app = FastAPI(title="Recruitment AI API", version="1.0.0")

# CORS
origins_env = os.getenv("ALLOW_ORIGINS", "*")
allow_origins = [o.strip() for o in origins_env.split(",")] if origins_env else ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins = ["http://localhost:3000", "https://recruit-ai-gms.netlify.app","https://recruit-ai-e055.onrender.com","http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

GOOGLE_CLIENT_ID = os.getenv("GOOGLE_CLIENT_ID")
GOOGLE_CLIENT_SECRET = os.getenv("GOOGLE_CLIENT_SECRET")
OAUTH_REDIRECT_URI = os.getenv("OAUTH_REDIRECT_URI")
FRONTEND_SUCCESS_URL = os.getenv("FRONTEND_SUCCESS_URL", "https://recruit-ai-gms.netlify.app")
GOOGLE_SCOPES = ["https://www.googleapis.com/auth/calendar"]

# ---------------------------------------------------------------------------
# Supabase
# ---------------------------------------------------------------------------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY in environment")

# BEFORE
# supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# AFTER
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")  # <- add this
if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY in environment")
if not SUPABASE_ANON_KEY:
    raise RuntimeError("Missing SUPABASE_ANON_KEY in environment")

admin_sb: Client = create_client(
    SUPABASE_URL,
    SUPABASE_SERVICE_KEY,
    options=ClientOptions(auto_refresh_token=False, persist_session=False),
)

public_sb: Client = create_client(
    SUPABASE_URL,
    SUPABASE_ANON_KEY,
    options=ClientOptions(auto_refresh_token=False, persist_session=False),
)

# Keep all your DB/storage code working without edits:
supabase: Client = admin_sb
# After your existing Supabase setup
SUPABASE_ENABLED = bool(SUPABASE_URL and SUPABASE_SERVICE_KEY)

INVITE_REDIRECT = os.getenv("INTERVIEWER_INVITE_REDIRECT")
RESET_REDIRECT  = os.getenv("PASSWORD_RESET_REDIRECT")
# ---------- NEW: storage + TTL settings ----------
RESUME_BUCKET = os.getenv("RESUME_BUCKET", "resumes")
RESUME_TTL_HOURS = int(os.getenv("RESUME_TTL_HOURS", "36"))
# ---------------------------------------------------------------------------
# Ollama (cloud-first)
# ---------------------------------------------------------------------------
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "https://ollama.com/api")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss:20b")
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY")

security = HTTPBearer()
# ---------------------------------------------------------------------------
# Pydantic models
# ---------------------------------------------------------------------------
class SignUpRequest(BaseModel):
    email: EmailStr
    password: str

class SignInRequest(BaseModel):
    email: EmailStr
    password: str

class CompanyRequest(BaseModel):
    company_name: str
    recruiter_name: str
    location: Optional[str] = None
    linkedin_url: Optional[str] = None
    description: Optional[str] = None

class JDCreateRequest(BaseModel):
    job_title: str
    min_experience: Optional[float] = None
    max_experience: Optional[float] = None
    company_name: str
    company_description: Optional[str] = None
    employment_type: str
    work_mode: str
    skills_must_have: List[str] = Field(default_factory=list)
    skills_nice_to_have: List[str] = Field(default_factory=list)
    requirements: str
    location: Optional[str] = None

class JDIngestText(BaseModel):
    jd_text: str

class JDIngestAnswer(BaseModel):
    original_jd_text: str
    parsed: Dict[str, Any]
    answers: Dict[str, Any]

class UserIdentity(BaseModel):
    user_id: str
    email: str

class ParseOneRequest(BaseModel):
    storage_key: Optional[str] = None
    file_hash: Optional[str] = None
    email_hint: Optional[str] = None  # optional override

class InterviewerInvite(BaseModel):
    name: str
    email: EmailStr
    is_active: Optional[bool] = True  # default True

class InterviewerUpdate(BaseModel):
    name: Optional[str] = None
    email: Optional[EmailStr] = None
    is_active: Optional[bool] = None

class SendResetOut(BaseModel):
    action_link: str  # helpful in dev; in prod you’ll email it, but still return it for UI fallback

class InterviewerOut(BaseModel):
    interviewer_id: str
    name: str
    email: EmailStr
    company_id: Optional[str] = None
    is_active: bool = True

# ---- Pydantic for interviewer auth ----
class InterviewerSignIn(BaseModel):
    email: EmailStr
    password: str

class InterviewerSessionOut(BaseModel):
    access_token: str
    refresh_token: str
    user: Dict[str, Any]
    interviewer: Dict[str, Any]

class BulkShortlistRequest(BaseModel):
    resume_ids: Optional[List[str]] = None          # shortlist by resume_id(s)
    emails: Optional[List[EmailStr]] = None         # or by candidate email(s)
    only_if_status: str = "PARSED"                  # safety: only update when current status == PARSED

class ScheduleInterviewIn(BaseModel):
    job_id: str
    resume_id: str
    interviewer_id: str
    start_iso: str
    end_iso: str
    timezone: str = "Asia/Kolkata"
    external_id: Optional[str] = None  # for idempotency

from typing import Literal

class CopilotQuestionIn(BaseModel):
    transcript_so_far: Optional[str] = None
    focus: Optional[str] = Field(
        default=None,
        description="Optional focus area like 'backend', 'system design', 'communication'"
    )

class CopilotAnswerIn(BaseModel):
    question: str
    raw_answer: str
    dimension: Optional[str] = Field(
        default=None,
        description="Optional rubric dimension, e.g. 'problem_solving', 'communication'"
    )

class CopilotFinalIn(BaseModel):
    notes: Optional[str] = Field(
        default=None,
        description="Free-form interviewer notes or transcript snippets"
    )
    decision: Optional[str] = Field(
        default=None,
        description="Interviewer decision: e.g. 'strong hire', 'hire', 'no hire', 'unsure'"
    )
    ratings: Optional[Dict[str, int]] = Field(
        default=None,
        description="Optional numeric ratings per competency, e.g. {'problem_solving': 4, 'communication': 3}"
    )

# -------- Interview feedback & recruiter decision models --------

from typing import Literal  # you already imported this above, so skip if present

class InterviewFeedbackIn(BaseModel):
    # Radio buttons from interviewer UI
    overall_recommendation: Literal["STRONG_HIRE", "HIRE", "UNSURE", "DO_NOT_HIRE"]
    skills_level: Literal["LOW", "MEDIUM", "HIGH"]
    communication_level: Literal["NEEDS_WORK", "OKAY", "STRONG"]
    jd_fit_level: Literal["PERFECT", "GOOD", "PARTIAL", "UNCERTAIN"]

    # Checkboxes: will map your UI keys → booleans
    # e.g. { strong_tech_depth: true, collab_mindset: true, needs_clarity: false, limited_data: false }
    flags: Dict[str, bool] = Field(default_factory=dict)

    # Free-text comment from interviewer
    comment: Optional[str] = None


class RecruiterDecisionIn(BaseModel):
    # What recruiter decides for this interview
    status: Literal["REVIEW", "HIRED", "REJECTED", "SELECTED"]
    note: Optional[str] = None  # recruiter internal note

class HiredEmailIn(BaseModel):
    start_iso: str        # joining start datetime with tz (e.g. 2025-12-01T10:00:00+05:30)
    end_iso: str          # end of that first day
    location: str
    timezone: str = "Asia/Kolkata"


class AssignmentEmailIn(BaseModel):
    deadline_iso: str     # assignment deadline datetime
    link: str
    description: str
    timezone: str = "Asia/Kolkata"


class RejectEmailIn(BaseModel):
    # Optional; if not provided, we'll default to "today" as an all-day event
    date: Optional[str] = None   # YYYY-MM-DD
    timezone: str = "Asia/Kolkata"

# ---------------------------------------------------------------------------
# Auth helpers
# ---------------------------------------------------------------------------
def verify_token(token: str) -> Optional[dict]:
    try:
        return jwt.decode(token, options={"verify_signature": False})
    except Exception:
        return None

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security),
) -> UserIdentity:
    token = credentials.credentials
    payload = verify_token(token)
    if not payload:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid authentication credentials")

    user_id = payload.get("sub")
    email = payload.get("email")
    if not user_id or not email:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token payload")

    return UserIdentity(user_id=user_id, email=email)

SUPABASE_URL = os.environ["SUPABASE_URL"]
# Use your public anon key here (same one you use on the frontend)
SUPABASE_ANON_KEY = os.environ["SUPABASE_ANON_KEY"]
RESET_REDIRECT = os.getenv("RESET_REDIRECT", "https://recruit-ai-gms.netlify.app/reset")

def _send_supabase_password_reset(email: str) -> None:
    recover_url = SUPABASE_URL.rstrip("/") + "/auth/v1/recover"

    resp = requests.post(
        recover_url,
        json={"email": email, "redirect_to": RESET_REDIRECT},
        headers={
            "apikey": SUPABASE_ANON_KEY,
            "Authorization": f"Bearer {SUPABASE_ANON_KEY}",
            "Content-Type": "application/json",
        },
        timeout=10,
    )

    if resp.status_code >= 400:
        # You can log resp.text for more detail
        raise HTTPException(
            status_code=500,
            detail=f"Supabase password reset failed with {resp.status_code}",
        )
# ---------------------------------------------------------------------------
# Ollama helpers
# ---------------------------------------------------------------------------
def _is_cloud_host(base_url: str) -> bool:
    return base_url.startswith("https://ollama.com")

def _normalize_model_tag(model: str, cloud: bool) -> str:
    if cloud and model.endswith("-cloud"):
        return model[:-6]
    return model

def generate_jd_with_ollama(prompt: str) -> str:
    try:
        is_cloud = _is_cloud_host(OLLAMA_BASE_URL)
        model = _normalize_model_tag(OLLAMA_MODEL, is_cloud)

        url = f"{OLLAMA_BASE_URL.rstrip('/')}/generate"
        headers = {"Content-Type": "application/json"}
        if is_cloud:
            if not OLLAMA_API_KEY:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Missing OLLAMA_API_KEY for Ollama Cloud",
                )
            headers["Authorization"] = f"Bearer {OLLAMA_API_KEY}"

        payload = {"model": model, "prompt": prompt, "stream": False}
        resp = requests.post(url, headers=headers, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        text = data.get("response")
        if not text:
            raise HTTPException(status_code=502, detail="Ollama returned empty response")
        return text
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ollama error: {e}")

# ---------------------------------------------------------------------------
# JD prompt
# ---------------------------------------------------------------------------
def create_jd_prompt(req: JDCreateRequest) -> str:
    skills_must = ", ".join(req.skills_must_have) if req.skills_must_have else ""
    skills_nice = ", ".join(req.skills_nice_to_have) if req.skills_nice_to_have else ""

    if req.min_experience is not None or req.max_experience is not None:
        min_exp = req.min_experience if req.min_experience is not None else 0
        max_exp = req.max_experience if req.max_experience is not None else "+"
        exp_range = f"{min_exp}-{max_exp} years"
    else:
        exp_range = ""

    work_mode_map = {"online": "Remote", "offline": "Onsite", "hybrid": "Hybrid"}
    work_mode = work_mode_map.get((req.work_mode or "").lower(), req.work_mode or "")

    prompt = f"""
You are an expert technical recruiter and job-description writer. Using the inputs below, write a polished, LinkedIn-ready job description.
Do not invent facts. If something isn’t provided, omit it—never write “TBD” or “put your name here.” Return only the final JD text.

Inputs:
- Job Title: {req.job_title}
- Company: {req.company_name}
- Company Description: {req.company_description or ""}
- Location: {req.location or ""}
- Employment Type: {req.employment_type}
- Work Mode: {work_mode}
- Experience Required: {exp_range}
- Must-Have Skills: {skills_must}
- Nice-to-Have Skills: {skills_nice}
- Additional Requirements: {req.requirements}

Write in a professional, inclusive tone. Use strong action verbs and avoid jargon. Structure the JD exactly as follows (markdown headings + bullets).
If “Company Description” is empty, omit the “About {req.company_name}” section.
Include “Tools & Technologies” only if the skills contain concrete technologies; otherwise omit it.

**{req.job_title} — {req.company_name}**

**About {req.company_name}**
(2–3 concise sentences using Company Description.)

**Role Overview**
(3–5 sentences that weave in Work Mode, Location, Employment Type, and Experience Required.)

**Key Responsibilities**
- 7–9 bullets tailored to the role and Additional Requirements.

**Required Qualifications**
- Bullets that reflect Experience Required and all Must-Have Skills.

**Preferred Qualifications**
- Bullets derived from Nice-to-Have Skills.

**Tools & Technologies** (optional)
- Comma-separated list from the skills above (only if meaningful).

Formatting rules:
- No preamble, no code fences, no instructions—only the final JD.
- Don’t state salary/benefits unless they appear in the inputs; otherwise omit those sections.
- Keep sentences crisp; avoid filler; no placeholders.
""".strip()
    return prompt

# ---------------------------------------------------------------------------
# tiny JD text parser for /jd/ingest-text
# ---------------------------------------------------------------------------
def normalize_work_mode(raw: Optional[str]) -> str:
    """Map a free-text work mode to one of: online | offline | hybrid."""
    if not raw:
        return "online"
    r = raw.strip().lower()

    # common remote patterns
    if any(k in r for k in ["remote", "wfh", "from home", "anywhere"]):
        return "online"

    # common onsite patterns
    if any(k in r for k in ["onsite", "on-site", "office", "in office"]):
        return "offline"

    # hybrid
    if "hybrid" in r:
        return "hybrid"

    # fallback
    return "online"

def normalize_employment_type(raw: Optional[str]) -> str:
    if not raw:
        return "Full-time"
    r = raw.strip().lower()
    if "part" in r:
        return "Part-time"
    if "contract" in r or "freelance" in r:
        return "Contract"
    if "intern" in r or "trainee" in r:
        return "Internship"
    return "Full-time"

def _extract_skills(block: str) -> List[str]:
    if not block:
        return []
    parts = re.split(r"[;,]", block)
    out = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # drop leading "Must have", etc
        p = re.sub(r"^(must|nice|good)\s*(to)?\s*have:?","",p,flags=re.I).strip()
        if p:
            out.append(p)
    return out

def _extract_fields_from_jd_text(jd_text: str) -> Dict[str, Any]:
    """
    Hybrid JD parser:
      1) Run old heuristic parsing (regex-based).
      2) Run AI parser (parse_jd_with_ai).
      3) Merge: prefer AI where it's confident, fall back to heuristic.
    """
    text = jd_text.strip()

    # ---------- 1) OLD HEURISTIC LOGIC (your existing code) ----------
    title = None
    m = re.search(r"(?i)^(?:we\s+are\s+looking\s+for\s+a|hiring\s+a|role:)?\s*([A-Z][\w\s\/\-\+]{4,50})", text)
    if m:
        title = m.group(1).strip(" .,")

    company = None
    m = re.search(r"(?i)(?:at|@)\s+([A-Z][\w\s\.\-&]{2,60})", text)
    if m:
        company = m.group(1).strip(" .,")

    location = None
    m = re.search(r"(?i)location\s*[:\-]\s*([^\n\.]+)", text)
    if m:
        location = m.group(1).strip()

    employment = None
    m = re.search(r"(?i)(employment|type)\s*[:\-]\s*([^\n\.]+)", text)
    if m:
        employment = m.group(2).strip()

    work_mode = None
    m = re.search(r"(?i)work\s*mode\s*[:\-]\s*([^\n\.]+)", text)
    if m:
        work_mode = m.group(1).strip()
    else:
        # fallback: see if "Remote" appears anywhere
        if re.search(r"(?i)\bremote\b", text):
            work_mode = "remote"
        elif re.search(r"(?i)\bhybrid\b", text):
            work_mode = "hybrid"
        elif re.search(r"(?i)\b(?:onsite|on-site|office)\b", text):
            work_mode = "onsite"

    min_exp = None
    max_exp = None
    m = re.search(r"(?i)(\d+)\+?\s*(?:years|yrs)\s+of\s+experience", text)
    if m:
        try:
            min_exp = float(m.group(1))
        except Exception:
            min_exp = None

    must = []
    nice = []

    m = re.search(r"(?i)must\s+have\s*[:\-]\s*(.+)", text)
    if m:
        must = _extract_skills(m.group(1))

    m = re.search(r"(?i)nice\s+to\s+have\s*[:\-]\s*(.+)", text)
    if m:
        nice = _extract_skills(m.group(1))

    heuristic = {
        "job_title": title,
        "company_name": company,
        "location": location,
        "employment_type": employment,
        "work_mode": work_mode,
        "min_experience": min_exp,
        "max_experience": max_exp,
        "skills_must_have": must,
        "skills_nice_to_have": nice,
        "jd_text": text,
    }

    # ---------- 2) AI PARSER (best-effort) ----------
    ai = {}
    try:
        ai = parse_jd_with_ai(text)
    except Exception:
        ai = {}

    def pick(*values):
        """Return first non-empty / non-null value."""
        for v in values:
            if v is None:
                continue
            if isinstance(v, str) and not v.strip():
                continue
            if isinstance(v, (list, dict)) and not v:
                continue
            return v
        return None

    # numbers: prefer AI if it returned a number
    def pick_num(ai_val, base_val):
        if isinstance(ai_val, (int, float)):
            return float(ai_val)
        return base_val

    skills_must = pick(ai.get("skills_must_have"), heuristic.get("skills_must_have")) or []
    skills_nice = pick(ai.get("skills_nice_to_have"), heuristic.get("skills_nice_to_have")) or []

    result = {
        "job_title": pick(ai.get("job_title"), heuristic.get("job_title")),
        "company_name": pick(ai.get("company_name"), heuristic.get("company_name")),
        "location": pick(ai.get("location"), heuristic.get("location")),
        "employment_type": pick(ai.get("employment_type"), heuristic.get("employment_type")),
        "work_mode": pick(ai.get("work_mode"), heuristic.get("work_mode")),
        "min_experience": pick_num(ai.get("min_experience"), heuristic.get("min_experience")),
        "max_experience": pick_num(ai.get("max_experience"), heuristic.get("max_experience")),
        "skills_must_have": skills_must,
        "skills_nice_to_have": skills_nice,
        # keep full text
        "jd_text": text,
    }

    return result

def _detect_missing_fields(parsed: Dict[str, Any]) -> List[Dict[str, str]]:
    """Return a list of questions we should ask the recruiter."""
    q = []

    if not parsed.get("job_title"):
        q.append({"field": "job_title", "question": "What's the job title for this posting?"})

    if not parsed.get("company_name"):
        q.append({"field": "company_name", "question": "Which company is this role for?"})

    if not parsed.get("employment_type"):
        q.append({
            "field": "employment_type",
            "question": "Is it Full-time, Part-time, Contract or Internship?",
            "options": ["Full-time", "Part-time", "Contract", "Internship"],
        })

    if not parsed.get("work_mode"):
        q.append({
            "field": "work_mode",
            "question": "What's the work mode? (Remote / Onsite / Hybrid)",
            "options": ["online (remote)", "offline (onsite)", "hybrid"],
        })

    if parsed.get("min_experience") is None:
        q.append({"field": "min_experience", "question": "What is the minimum experience required (in years)?"})

    return q

def _download_from_storage(key: str) -> bytes:
    bucket = supabase.storage.from_(RESUME_BUCKET)
    return bucket.download(key)
def _get_job_owned(job_id: str, user: UserIdentity) -> dict:
    """
    Fetch a job that belongs to the current recruiter.
    Returns the job dict or raises 404 if not found / not owned.

    Uses .limit(1) + _first_row instead of .single() so we don't
    get PGRST116 when there are 0 rows.
    """
    resp = (
        supabase.table("jobs")
        .select("*")
        .eq("job_id", job_id)
        .eq("created_by", user.user_id)
        .limit(1)
        .execute()
    )
    job = _first_row(resp)

    if not job:
        # no job for this user → either wrong job_id or unauthorized
        raise HTTPException(404, "Job not found or unauthorized")

    return job

def _get_recruiter_interview_context(
    interview_id: str,
    current_user: UserIdentity,
) -> Dict[str, Any]:
    """
    Load interview + job + resume (+company) ensuring the current user owns the job.
    Returns dict: {interview, job, resume, company}
    """
    interview = _first_row(
        supabase.table("interviews")
        .select("*")
        .eq("interview_id", interview_id)
        .limit(1)
        .execute()
    )
    if not interview:
        raise HTTPException(404, "Interview not found")

    job = _get_job_owned(interview["job_id"], current_user)

    resume = _first_row(
        supabase.table("resumes")
        .select("resume_id, full_name, email")
        .eq("resume_id", interview["resume_id"])
        .limit(1)
        .execute()
    )

    company = None
    if job.get("company_id"):
        company = _first_row(
            supabase.table("companies")
            .select("company_name")
            .eq("company_id", job["company_id"])
            .limit(1)
            .execute()
        )

    return {
        "interview": interview,
        "job": job,
        "resume": resume,
        "company": company,
    }

def _hired_email_body(candidate_name: str, role: str, company_name: str,
                      start_iso: str, location: str) -> str:
    return f"""
Hi {candidate_name or "there"},

We are pleased to inform you that you have been selected for the role of {role} at {company_name}.

Your joining details are as follows:
- Start date & time: {start_iso}
- Location: {location}

Our HR team will share the remaining formalities and documentation shortly. 
If you have any questions or need clarification, feel free to reply to this email.

Welcome on board!

Best regards,
{company_name} Recruitment Team
""".strip()


def _rejected_email_body(candidate_name: str, role: str, company_name: str) -> str:
    return f"""
Hi {candidate_name or "there"},

Thank you for taking the time to interview for the {role} position at {company_name}.

After careful consideration, we will not be moving forward with your application at this time.
This decision was not easy, and it does not diminish your skills or experience.

We truly appreciate your interest in {company_name} and encourage you to apply for future roles that match your profile.

Wishing you all the best in your job search.

Best regards,
{company_name} Recruitment Team
""".strip()


def _assignment_email_body(candidate_name: str, role: str, company_name: str,
                           deadline_iso: str, link: str, description: str) -> str:
    return f"""
Hi {candidate_name or "there"},

As the next step for the {role} position at {company_name}, we would like you to complete a short assignment.

Details:
- Submission deadline: {deadline_iso}
- Assignment link: {link}

Instructions:
{description}

Please share your solution before the deadline. If you have any questions or need clarification, reply to this email.

Best of luck!

Best regards,
{company_name} Recruitment Team
""".strip()

def _get_company_id_for_user(user: UserIdentity) -> Optional[str]:
    rec = (
        supabase.table("recruiters")
        .select("company_id")
        .eq("user_id", user.user_id)
        .limit(1)
        .execute()
        .data
    )
    return rec[0]["company_id"] if rec else None

def _today_bounds_for_tz(tz_name: str = "Asia/Kolkata") -> tuple[datetime.date, str, str]:
    """
    Compute today's start & end in the given timezone, but return
    ISO strings in UTC with 'Z' suffix for Supabase .gte/.lt filters.

    Returns:
      (local_date, start_iso_utc, end_iso_utc)
    """
    # choose timezone (fallback to UTC if anything goes wrong)
    if ZoneInfo is not None:
        try:
            tz = ZoneInfo(tz_name)
        except Exception:
            tz = datetime.timezone.utc
    else:
        tz = datetime.timezone.utc

    now = datetime.datetime.now(tz)
    start_local = datetime.datetime(now.year, now.month, now.day, 0, 0, 0, tzinfo=tz)
    end_local = start_local + datetime.timedelta(days=1)

    start_utc = start_local.astimezone(datetime.timezone.utc)
    end_utc = end_local.astimezone(datetime.timezone.utc)

    # Format as ISO strings with Z suffix.
    return (
        start_local.date(),
        start_utc.isoformat().replace("+00:00", "Z"),
        end_utc.isoformat().replace("+00:00", "Z"),
    )

from postgrest.exceptions import APIError as PgAPIError  # you already import this later

def _get_interviewer_by_auth_user(user: UserIdentity) -> Dict[str, Any]:
    resp = (
        supabase.table("interviewers")
        .select("interviewer_id, auth_user_id, company_id, name, email, is_active")
        .eq("auth_user_id", user.user_id)
        .limit(1)
        .execute()
    )
    row = _first_row(resp)
    if not row:
        raise HTTPException(status_code=403, detail="Interviewer profile not found")
    if not row.get("is_active", True):
        raise HTTPException(status_code=403, detail="Interviewer is deactivated")
    return row


def _get_interview_context(
    interview_id: str,
    current_user: UserIdentity,
) -> Dict[str, Any]:
    """
    Load interview + job + resume for this interviewer.
    Ensures the interview belongs to the logged-in interviewer.
    """
    interviewer = _get_interviewer_by_auth_user(current_user)
    interviewer_id = interviewer["interviewer_id"]

    # Interview must belong to this interviewer
    interview = _first_row(
        supabase.table("interviews")
        .select(
            "interview_id, job_id, resume_id, status, start_at, end_at, "
            "google_meet_link, google_html_link"
        )
        .eq("interview_id", interview_id)
        .eq("interviewer_id", interviewer_id)
        .limit(1)
        .execute()
    )
    if not interview:
        raise HTTPException(status_code=404, detail="Interview not found for this interviewer")

    job = _first_row(
        supabase.table("jobs")
        .select(
            "job_id, role, location, employment_type, work_mode, jd_text, "
            "skills_must_have, skills_nice_to_have, min_years, max_years"
        )
        .eq("job_id", interview["job_id"])
        .limit(1)
        .execute()
    )

    resume_row = _first_row(
        supabase.table("resumes")
        .select(
            "resume_id, candidate_id, full_name, email, phone, location, "
            "raw_text, ai_summary, skills, experience, education, projects, "
            "certifications, jd_match_score, skill_match_score, "
            "experience_match_score, meta"
        )
        .eq("resume_id", interview["resume_id"])
        .limit(1)
        .execute()
    )

    if not job or not resume_row:
        raise HTTPException(status_code=404, detail="Job or resume missing for interview")

    # Convert skill lists safely if stored as JSON strings
    must = _safe_json_list(job.get("skills_must_have"))
    nice = _safe_json_list(job.get("skills_nice_to_have"))

    meta = (resume_row.get("meta") or {})
    return {
        "interviewer": interviewer,
        "interview": interview,
        "job": job,
        "resume": resume_row,
        "must": must,
        "nice": nice,
        "meta": meta,
    }

def _pkce_challenge(verifier: str) -> str:
    dig = hashlib.sha256(verifier.encode()).digest()
    return base64.urlsafe_b64encode(dig).rstrip(b"=").decode()

def _save_google_tokens(user_id: str, token: dict):
    # upsert token for this user
    supabase.table("google_oauth_tokens").upsert({
        "user_id": user_id,
        "refresh_token": token.get("refresh_token"),
        "access_token": token.get("access_token"),
        "token_expiry": datetime.datetime.utcfromtimestamp(
            datetime.datetime.utcnow().timestamp() + int(token.get("expires_in", 3600))
        ).isoformat() + "Z",
        "scope": token.get("scope"),
        "updated_at": datetime.datetime.utcnow().isoformat() + "Z",
    }).execute()

def _get_google_refresh_token(user_id: str) -> str | None:
    rows = (
        supabase.table("google_oauth_tokens")
        .select("refresh_token")
        .eq("user_id", user_id)
        .limit(1)
        .execute()
        .data
    )
    return rows[0]["refresh_token"] if rows else None


def _google_service_for_user(user_id: str):
    rtok = _get_google_refresh_token(user_id)
    if not rtok:
        raise HTTPException(400, "Google is not connected for this account")

    creds = Credentials(
        token=None,
        refresh_token=rtok,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
        scopes=GOOGLE_SCOPES,
    )
    # optional proactive refresh
    # creds.refresh(GARequest())
    return build("calendar", "v3", credentials=creds, cache_discovery=False)

def _first_row(resp):
    """Works across supabase-py versions: returns the first row or None."""
    if resp is None:
        return None
    data = getattr(resp, "data", None)
    if isinstance(data, list):
        return data[0] if data else None
    return data

# ─────────────────────────────────────────────
# Google STT helpers (MeetAI)
# ─────────────────────────────────────────────
MAX_STREAM_SECONDS = 290  # keep from meetai.py

def get_speech_client() -> speech.SpeechClient:
    """
    Uses GOOGLE_APPLICATION_CREDENTIALS if set, otherwise looks for service-account.json
    in the backend folder, otherwise falls back to ADC.
    """
    env_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if env_path and os.path.exists(env_path):
        creds = service_account.Credentials.from_service_account_file(env_path)
        print(f"[startup] using credentials from env: {env_path}")
        return speech.SpeechClient(credentials=creds)
    local = Path(__file__).with_name("service-account.json")
    if local.exists():
        creds = service_account.Credentials.from_service_account_file(str(local))
        print("[startup] using credentials from backend/service-account.json")
        return speech.SpeechClient(credentials=creds)
    print("[startup] using ADC")
    return speech.SpeechClient()

def make_recognition_config():
    return speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        enable_automatic_punctuation=True,
        use_enhanced=True,
        model="default",
    )

def make_streaming_config():
    return speech.StreamingRecognitionConfig(
        config=make_recognition_config(),
        interim_results=True,
        single_utterance=False,
    )
# ─────────────────────────────────────────────
# Meetings + transcript + ai_report helpers
# ─────────────────────────────────────────────

def _sb_get_meeting(session_id: str) -> dict:
    if not SUPABASE_ENABLED:
        return {}
    try:
        m = (
            supabase
            .table("meetings")
            .select("*")
            .eq("meeting_id", session_id)
            .limit(1)
            .execute()
        )
        data = getattr(m, "data", None)
        if data:
            return data[0]
    except Exception as e:
        print(f"[sb] get meeting error: {e}")
    return {}

def _sb_upd_meeting_transcript(session_id: str, new_transcript: str) -> None:
    if not SUPABASE_ENABLED:
        return
    try:
        supabase.table("meetings").update({"transcript": new_transcript}).eq(
            "meeting_id", session_id
        ).execute()
    except Exception as e:
        print(f"[sb] update transcript error: {e}")

def _append_transcript_line(session_id: str, speaker_name: str, text: str) -> None:
    mtg = _sb_get_meeting(session_id)
    prev = mtg.get("transcript") or ""
    line = f"{speaker_name}: {text}".strip()
    new = (prev.rstrip() + "\n" + line) if prev.strip() else line
    _sb_upd_meeting_transcript(session_id, new)

def _sb_upsert_ai_report(session_id: str, report: dict) -> None:
    """
    Store AI summary json into meetings.ai_report (jsonb).
    """
    if not SUPABASE_ENABLED:
        return
    try:
        safe_report = json.loads(json.dumps(report, ensure_ascii=False))
    except Exception as e:
        print(f"[sb] ai_report not JSON-serializable: {e}")
        return
    try:
        supabase.table("meetings").update({"ai_report": safe_report}).eq(
            "meeting_id", session_id
        ).execute()
    except Exception as e:
        print(f"[sb] update meetings.ai_report error: {e}")

# ─────────────────────────────────────────────
# MeetAI LLM helper – reuse generate_jd_with_ollama
# ─────────────────────────────────────────────

def _ollama_chat_json(prompt: str, temperature: float = 0.2) -> dict:
    """
    Call your existing generate_jd_with_ollama() (which hits Ollama HTTP /generate)
    and try to parse the response as JSON.
    If parsing fails, return {"raw": text}.
    """
    # you can modify generate_jd_with_ollama to accept temperature if you want;
    # for now we just ignore the temperature param.
    text = generate_jd_with_ollama(prompt)
    text = (text or "").strip()
    try:
        return json.loads(text)
    except Exception:
        return {"raw": text}

# ─────────────────────────────────────────────
# Formatting helpers (bullets, etc.)
# ─────────────────────────────────────────────
_CODE_FENCE = re.compile(r"```.*?```", flags=re.S)

def _strip_code_and_json(s: str) -> str:
    if not s:
        return ""
    s = _CODE_FENCE.sub("", s)
    s = s.replace("`", " ")
    s = re.sub(r"(?m)^\s{0,3}#{1,6}\s*", "", s)
    s = re.sub(r"\*+", "", s)
    s = re.sub(r"_+", "", s)
    s = s.strip()
    if s.startswith("{") and s.endswith("}"):
        try:
            obj = json.loads(s)
            s = obj.get("explanation") or obj.get("expected_answer") or ""
        except Exception:
            s = ""
    return s.strip()

def _to_bullets(s: str, max_items: int = 6, max_len: int = 180) -> str:
    s = _strip_code_and_json(s)
    if not s:
        return ""
    parts = re.split(r"[\r\n]+|(?:^|[\s])[-–•·]\s+", s)
    parts = [p.strip(" -–•\t\r\n") for p in parts if p and p.strip()]
    if len(parts) <= 1:
        parts = re.split(r"(?<=[.!?])\s+", s)
    clean = []
    for p in parts:
        p = re.sub(r"\s+", " ", p).strip()
        if not p:
            continue
        if len(p) > max_len:
            p = p[: max_len - 1].rstrip() + "…"
        clean.append(p)
        if len(clean) >= max_items:
            break
    if not clean:
        return ""
    return "\n".join(f"• {p}" for p in clean)

# Transcript → pseudo turns
def _parse_transcript_to_turns(
    transcript_text: str, interviewer_name: str, candidate_name: str
) -> list[dict]:
    if not (transcript_text or "").strip():
        return []
    out: list[dict] = []
    lines = [ln.strip() for ln in transcript_text.splitlines() if ln.strip()]
    for ln in lines:
        if ":" in ln:
            name, msg = ln.split(":", 1)
            name = name.strip()
            msg = msg.strip()
        else:
            if out:
                out[-1]["text"] += " " + ln
                continue
            name, msg = "Unknown", ln

        role = "unknown"
        if interviewer_name and name.lower() == interviewer_name.lower():
            role = "interviewer"
        elif candidate_name and name.lower() == candidate_name.lower():
            role = "candidate"
        else:
            if name.lower() in ("interviewer", "mic"):
                role = "interviewer"
            elif name.lower() in ("candidate", "tab"):
                role = "candidate"

        out.append(
            {
                "source": "mic"
                if role == "interviewer"
                else ("tab" if role == "candidate" else "unknown"),
                "speaker": role,
                "speaker_name": name,
                "text": msg,
                "is_final": True,
            }
        )
    return out

def google_stt_worker(
    source: str,
    session_id: Optional[str],
    speaker: str,
    speaker_name: str,
    client: speech.SpeechClient,
    audio_q: "queue.Queue[Optional[bytes]]",
    loop: asyncio.AbstractEventLoop,
    out_q: "asyncio.Queue[Optional[dict]]",
):
    def gen_with_config(stop_flag: dict):
        yield speech.StreamingRecognizeRequest(
            streaming_config=make_streaming_config()
        )
        while True:
            chunk = audio_q.get()
            if chunk is None:
                stop_flag["stop"] = True
                break
            yield speech.StreamingRecognizeRequest(audio_content=chunk)

    def audio_only(stop_flag: dict):
        while True:
            chunk = audio_q.get()
            if chunk is None:
                stop_flag["stop"] = True
                break
            yield speech.StreamingRecognizeRequest(audio_content=chunk)

    while True:
        stop_flag = {"stop": False}
        start = time.time()
        try:
            try:
                responses = client.streaming_recognize(
                    requests=gen_with_config(stop_flag)
                )
            except TypeError:
                responses = client.streaming_recognize(
                    make_streaming_config(), audio_only(stop_flag)
                )

            for resp in responses:
                if not resp.results:
                    if time.time() - start > MAX_STREAM_SECONDS:
                        break
                    continue
                for result in resp.results:
                    text = result.alternatives[0].transcript
                    is_final = result.is_final

                    asyncio.run_coroutine_threadsafe(
                        out_q.put(
                            {
                                "source": source,
                                "speaker": speaker,
                                "speaker_name": speaker_name,
                                "text": text,
                                "final": is_final,
                            }
                        ),
                        loop,
                    )

                    if (
                        SUPABASE_ENABLED
                        and session_id
                        and is_final
                        and text.strip()
                    ):
                        try:
                            _append_transcript_line(
                                session_id, speaker_name, text.strip()
                            )
                        except Exception as e:
                            print(f"[stt:{source}] transcript append error: {e}")

                if time.time() - start > MAX_STREAM_SECONDS:
                    print("[stt] rotating stream to avoid 305s limit…")
                    break

        except Exception as e:
            if "Audio Timeout Error" in str(e):
                print("[stt] timeout; restarting stream…")
                continue
        finally:
            if stop_flag["stop"]:
                break

    asyncio.run_coroutine_threadsafe(out_q.put(None), loop)

async def result_sender(
    websocket: WebSocket, out_q: "asyncio.Queue[Optional[dict]]", source: str
):
    while True:
        item = await out_q.get()
        if item is None:
            break
        try:
            await websocket.send_text(json.dumps(item))
        except Exception as e:
            print(f"[ws:{source}] send error: {e}")
            break

def _recent_turns(session_id: str, limit: int = 2000) -> list[dict]:
    """
    Load the meeting transcript from Supabase and convert it into
    pseudo-turns (speaker + text) using _parse_transcript_to_turns.
    The 'limit' is a soft cap on number of characters; you can
    adjust if you want.
    """
    mtg = _sb_get_meeting(session_id)
    if not mtg:
        return []

    transcript = mtg.get("transcript") or ""
    interviewer_name = mtg.get("interviewer_name") or ""
    candidate_name = mtg.get("candidate_name") or ""

    # If transcript is very long, truncate from the end (most recent lines)
    if len(transcript) > limit:
        # keep last `limit` chars
        transcript = transcript[-limit:]

    return _parse_transcript_to_turns(
        transcript_text=transcript,
        interviewer_name=interviewer_name,
        candidate_name=candidate_name,
    )

# ─────────────────────────────────────────────
# Summary payload coercer — ALWAYS return requested JSON shape
# ─────────────────────────────────────────────
def _coerce_summary_payload(model_out: Any) -> dict:
    """
    Normalizes the model output into the strict summary shape:

    {
      "analytics": {
        "skills_match": int 0-10,
        "communication_experience": int 0-10,
        "jd_alignment": int 0-10,
        "overall_score": int 0-10  # computed as average of the three above
      },
      "role_fit": "...",
      "experience": "...",
      "strengths": "...",
      "weaknesses": "...",
      "interview_summary": "..."
    }
    """
    out = {
        "analytics": {
            "skills_match": 0,
            "communication_experience": 0,
            "jd_alignment": 0,
            "overall_score": 0,
        },
        "role_fit": "uncertain",
        "experience": "",
        "strengths": "",
        "weaknesses": "",
        "interview_summary": "",
    }

    # If the model just sent a string, treat it as a generic interview summary
    if isinstance(model_out, str):
        out["interview_summary"] = model_out.strip()
        return out

    if not isinstance(model_out, dict):
        return out

    analytics = model_out.get("analytics") or {}

    def _to_float(x, default=0.0):
        try:
            return float(x)
        except Exception:
            return default

    def _clamp10(v: float) -> float:
        v = float(v)
        if v < 0:
            v = 0.0
        if v > 10:
            v = 10.0
        return v

    # Pull scores (allowing for a few alternative names just in case)
    skills_match = _to_float(analytics.get("skills_match"))
    comm_exp = _to_float(
        analytics.get("communication_experience")
        or analytics.get("communication")
        or analytics.get("communication_score")
    )
    jd_alignment = _to_float(
        analytics.get("jd_alignment")
        or analytics.get("jd_fit")
        or analytics.get("role_fit_score")
    )

    skills_match = _clamp10(skills_match)
    comm_exp = _clamp10(comm_exp)
    jd_alignment = _clamp10(jd_alignment)

    out["analytics"]["skills_match"] = int(round(skills_match))
    out["analytics"]["communication_experience"] = int(round(comm_exp))
    out["analytics"]["jd_alignment"] = int(round(jd_alignment))

    # overall_score MUST be the average of the three, if we have any signal
    scores = [skills_match, comm_exp, jd_alignment]
    nonzero = [s for s in scores if s > 0]
    if nonzero:
        overall = sum(nonzero) / len(nonzero)
    else:
        # fall back to model's own overall_score if absolutely nothing else is set
        overall = _clamp10(_to_float(analytics.get("overall_score"), 0.0))
    out["analytics"]["overall_score"] = int(round(overall))

    # role_fit: normalize into a small set of labels
    role_fit = (model_out.get("role_fit") or "").strip().lower()
    allowed_role_fits = {
        "perfect fit",
        "good fit",
        "partial fit",
        "uncertain",
        "not a fit",
    }
    if role_fit not in allowed_role_fits:
        # Try to infer roughly, otherwise keep "uncertain"
        if "perfect" in role_fit:
            role_fit = "perfect fit"
        elif "good" in role_fit:
            role_fit = "good fit"
        elif "partial" in role_fit:
            role_fit = "partial fit"
        elif "not" in role_fit:
            role_fit = "not a fit"
        else:
            role_fit = "uncertain"
    out["role_fit"] = role_fit

    # experience: just keep the text the model sends (e.g. fresher / mid level / senior)
    out["experience"] = str(model_out.get("experience") or "").strip()

    def _coerce_text(v: Any) -> str:
        if isinstance(v, list):
            return " ".join(str(x).strip() for x in v if str(x).strip())
        return str(v or "").strip()

    out["strengths"] = _coerce_text(model_out.get("strengths"))
    out["weaknesses"] = _coerce_text(model_out.get("weaknesses"))
    out["interview_summary"] = _coerce_text(model_out.get("interview_summary"))

    return out
# ─────────────────────────────────────────────
# Interview feedback → numeric scores (0–10)
# ─────────────────────────────────────────────

_OVERALL_MAP = {
    "STRONG_HIRE": 10,
    "HIRE": 8,
    "UNSURE": 5,
    "DO_NOT_HIRE": 2,
}

_SKILLS_MAP = {
    "LOW": 3,
    "MEDIUM": 6,
    "HIGH": 9,
}

_COMM_MAP = {
    "NEEDS_WORK": 3,
    "OKAY": 6,
    "STRONG": 9,
}

_JD_FIT_MAP = {
    "PERFECT": 10,
    "GOOD": 8,
    "PARTIAL": 5,
    "UNCERTAIN": 3,
}


def compute_feedback_scores(body: InterviewFeedbackIn) -> Dict[str, int]:
    """Map radio selections to consistent 0–10 scores."""
    skills = _SKILLS_MAP[body.skills_level]
    comm = _COMM_MAP[body.communication_level]
    jd   = _JD_FIT_MAP[body.jd_fit_level]

    # base from 3 dimensions
    base_overall = round((skills + comm + jd) / 3)

    # interviewer recommendation nudges overall
    rec = _OVERALL_MAP[body.overall_recommendation]
    overall = round(0.7 * base_overall + 0.3 * rec)

    # clamp
    overall = max(0, min(10, overall))

    return {
        "skills_score": skills,
        "communication_score": comm,
        "jd_fit_score": jd,
        "overall_score": overall,
    }

def final_candidate_score(
    jd_match: Optional[float],
    feedback_overall: Optional[int],
    ai_overall: Optional[int],
) -> float:
    """
    Combine JD match (0–100) + feedback (0–10) + AI overall (0–10)
    into one final score on 0–10 for ranking.
    """
    jd = (jd_match or 0.0) / 10.0  # normalize 0–10
    fb = float(feedback_overall or 0)
    ai = float(ai_overall or 0)

    # weights: tweak as you like (must sum to 1)
    score = 0.4 * jd + 0.4 * fb + 0.2 * ai
    return round(score, 2)

# ─────────────────────────────────────────────
# Candidate answer extraction (transcript-only)
# ─────────────────────────────────────────────
def _norm(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"[^a-z0-9\s]+", "", s)
    s = re.sub(r"\s+", " ", s)
    return s


def _is_interviewer(t: dict, interviewer_name: str) -> bool:
    who = (t.get("speaker") or t.get("source") or "").lower()
    name = (t.get("speaker_name") or "").lower()
    if "interviewer" in who or "mic" in who:
        return True
    if interviewer_name and name and name == interviewer_name.lower():
        return True
    return False


def _is_candidate(t: dict, candidate_name: str) -> bool:
    who = (t.get("speaker") or t.get("source") or "").lower()
    name = (t.get("speaker_name") or "").lower()
    if "candidate" in who or "tab" in who:
        return True
    if candidate_name and name and name == candidate_name.lower():
        return True
    return False


def _turns_snippet(turns: List[dict], last_n: int = 12) -> str:
    ctx = turns[-last_n:] if len(turns) > last_n else turns[:]
    lines = []
    for t in ctx:
        who = t.get("speaker_name") or t.get("speaker") or t.get("source", "unknown")
        text = t.get("text", "")
        lines.append(f"{who}: {text}")
    return "\n".join(lines)


def _extract_candidate_answer(
    question_text: str,
    turns: List[dict],
    interviewer_name: str = "",
    candidate_name: str = "",
) -> str:
    if not turns:
        return ""

    qn = _norm(question_text)
    best_idx, best_score = None, 0.0
    qtoks = set(qn.split())

    # Find the interviewer turn that best matches the question
    for i, t in enumerate(turns):
        if not _is_interviewer(t, interviewer_name):
            continue
        tn = _norm(t.get("text", ""))
        if not tn:
            continue
        ttoks = set(tn.split())
        if not qtoks or not ttoks:
            continue
        overlap = len(qtoks & ttoks) / max(1, len(qtoks))
        if qn in tn or tn in qn:
            overlap = 1.0
        if overlap > best_score:
            best_score, best_idx = overlap, i

    # Collect candidate's answer after that interviewer turn
    if best_idx is not None:
        out = []
        for j in range(best_idx + 1, len(turns)):
            t = turns[j]
            if _is_interviewer(t, interviewer_name):
                break
            if _is_candidate(t, candidate_name):
                out.append(t.get("text", ""))
            if len(out) >= 6:
                break
        return " ".join(x.strip() for x in out if x.strip())

    # Fallback: last few candidate turns
    cand = [t.get("text", "") for t in turns[-10:] if _is_candidate(t, candidate_name)]
    return " ".join(x.strip() for x in cand if x.strip())

# ---------------------------------------------------------------------------
# routes (YOUR ORIGINAL ROUTES — UNCHANGED)
# ---------------------------------------------------------------------------
@app.get("/")
async def root():
    return {"message": "Recruitment AI API is running", "status": "active"}

# ---------- auth ----------
@app.post("/auth/signup")
async def signup(request: SignUpRequest):
    try:
        resp = public_sb.auth.sign_up({"email": request.email, "password": request.password})
        if resp.user:
            return {"message": "User created successfully", "user": {"id": resp.user.id, "email": resp.user.email}}
        raise HTTPException(status_code=400, detail="Failed to create user")
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/auth/signin")
async def signin(request: SignInRequest):
    try:
        resp = public_sb.auth.sign_in_with_password({"email": request.email, "password": request.password})
        if resp.session:
            return {
                "access_token": resp.session.access_token,
                "refresh_token": resp.session.refresh_token,
                "user": {"id": resp.user.id, "email": resp.user.email},
            }
        raise HTTPException(status_code=401, detail="Invalid credentials")
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid email or password")

# Start OAuth (must be called by a logged-in user)
from fastapi.responses import JSONResponse

@app.get("/auth/google/start")
async def google_start(current_user: UserIdentity = Depends(get_current_user)):
    if not GOOGLE_CLIENT_ID or not GOOGLE_CLIENT_SECRET or not OAUTH_REDIRECT_URI:
        raise HTTPException(500, "Google OAuth not configured")

    # PKCE + state
    state = secrets.token_urlsafe(24)
    verifier = secrets.token_urlsafe(64)

    # build Google OAuth URL directly here
    params = {
        "client_id": GOOGLE_CLIENT_ID,
        "redirect_uri": OAUTH_REDIRECT_URI,
        "response_type": "code",
        "scope": " ".join(GOOGLE_SCOPES),
        "state": state,
        "access_type": "offline",
        "prompt": "consent",
        "code_challenge": _pkce_challenge(verifier),
        "code_challenge_method": "S256",
    }
    url = "https://accounts.google.com/o/oauth2/v2/auth?" + urlencode(params)

    # respond with JSON + set cookies
    resp = JSONResponse({"auth_url": url})
    resp.set_cookie("g_state", state, httponly=True,
                    samesite=COOKIE_SAMESITE, secure=COOKIE_SECURE)
    resp.set_cookie("g_verifier", verifier, httponly=True,
                    samesite=COOKIE_SAMESITE, secure=COOKIE_SECURE)
    resp.set_cookie("sb_uid", current_user.user_id, httponly=True,
                    samesite=COOKIE_SAMESITE, secure=COOKIE_SECURE)
    return resp


# Google's redirect URI (MUST match in Google console)
@app.get("/oauth2/callback")
async def oauth_callback(request: Request, code: str, state: str):
    st = request.cookies.get("g_state")
    verifier = request.cookies.get("g_verifier")
    user_id = request.cookies.get("sb_uid")
    if not st or st != state or not verifier or not user_id:
        raise HTTPException(400, "Invalid state or session")

    data = {
        "client_id": GOOGLE_CLIENT_ID,
        "client_secret": GOOGLE_CLIENT_SECRET,
        "code": code,
        "code_verifier": verifier,
        "grant_type": "authorization_code",
        "redirect_uri": OAUTH_REDIRECT_URI,
    }
    async with httpx.AsyncClient() as client:
        r = await client.post("https://oauth2.googleapis.com/token", data=data, timeout=30)

    try:
        token = r.json()
    except Exception:
        raise HTTPException(400, f"Token HTTP {r.status_code}: {r.text}")

    if "error" in token:
        # surfaces 'invalid_client', 'redirect_uri_mismatch', etc.
        raise HTTPException(400, f"Google token error: {token.get('error')} - {token.get('error_description')}")

    saved_rt = _get_google_refresh_token(user_id)
    if "refresh_token" not in token:
        if saved_rt:
            token["refresh_token"] = saved_rt
        else:
            raise HTTPException(400, "Missing refresh_token (ask user to check 'consent' and offline access)")

    _save_google_tokens(user_id, token)

    # clean cookies and send back to frontend
    resp = RedirectResponse(url=FRONTEND_SUCCESS_URL)
    resp.delete_cookie("g_state")
    resp.delete_cookie("g_verifier")
    resp.delete_cookie("sb_uid")
    return resp

@app.get("/auth/google/status")
def google_status(current_user: UserIdentity = Depends(get_current_user)):
    return {"connected": bool(_get_google_refresh_token(current_user.user_id))}

# ---------- me ----------
@app.get("/me")
async def get_me(current_user: UserIdentity = Depends(get_current_user)):
    recruiter_resp = (
        supabase.table("recruiters")
        .select("user_id, full_name, company_id, company_email")
        .eq("user_id", current_user.user_id)
        .execute()
    )

    if recruiter_resp.data:
        recruiter = recruiter_resp.data[0]
        company = None
        if recruiter.get("company_id"):
            company_resp = (
                supabase.table("companies")
                .select("company_id, company_name, description, location, linkedin_url")
                .eq("company_id", recruiter["company_id"])
                .execute()
            )
            if company_resp.data:
                company = company_resp.data[0]
        return {
            "user": {"id": current_user.user_id, "email": current_user.email},
            "recruiter": recruiter,
            "company": company,
            "has_company": bool(company),
        }

    return {
        "user": {"id": current_user.user_id, "email": current_user.email},
        "recruiter": None,
        "company": None,
        "has_company": False,
    }

@app.get("/recruiter/interviews/today")
async def list_recruiter_interviews_today(
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    For the logged-in recruiter, list INTERVIEW_SCHEDULED interviews for *today*
    across all jobs they created.

    Returns: candidate name, job title, interviewer name, date, time, status, meet link.
    """
    # 1) All jobs created by this recruiter
    jobs = (
        supabase.table("jobs")
        .select("job_id, role")
        .eq("created_by", current_user.user_id)
        .execute()
        .data
        or []
    )
    if not jobs:
        return {
            "date": datetime.date.today().isoformat(),
            "count": 0,
            "interviews": [],
        }

    job_ids = [j["job_id"] for j in jobs]
    jobs_by_id = {j["job_id"]: j for j in jobs}

    # 2) Today's window in IST, converted to UTC
    day, start_iso, end_iso = _today_bounds_for_tz("Asia/Kolkata")

    # 3) Interviews for those jobs today, scheduled
    rows = (
        supabase.table("interviews")
        .select(
            "interview_id, job_id, resume_id, interviewer_id, "
            "status, start_at, end_at, google_meet_link, google_html_link"
        )
        .in_("job_id", job_ids)
        .eq("status", "INTERVIEW_SCHEDULED")
        .gte("start_at", start_iso)
        .lt("start_at", end_iso)
        .order("start_at", desc=False)
        .execute()
        .data
        or []
    )

    if not rows:
        return {
            "date": day.isoformat(),
            "count": 0,
            "interviews": [],
        }

    # 4) Bulk fetch resumes (candidate) and interviewers
    resume_ids = sorted({r["resume_id"] for r in rows if r.get("resume_id")})
    interviewer_ids = sorted({r["interviewer_id"] for r in rows if r.get("interviewer_id")})

    resumes_by_id: Dict[str, Dict[str, Any]] = {}
    interviewers_by_id: Dict[str, Dict[str, Any]] = {}

    if resume_ids:
        res_rows = (
            supabase.table("resumes")
            .select("resume_id, full_name, email")
            .in_("resume_id", resume_ids)
            .execute()
            .data
            or []
        )
        resumes_by_id = {r["resume_id"]: r for r in res_rows}

    if interviewer_ids:
        intv_rows = (
            supabase.table("interviewers")
            .select("interviewer_id, name, email")
            .in_("interviewer_id", interviewer_ids)
            .execute()
            .data
            or []
        )
        interviewers_by_id = {i["interviewer_id"]: i for i in intv_rows}

    items = []
    for r in rows:
        job = jobs_by_id.get(r["job_id"])
        res = resumes_by_id.get(r["resume_id"])
        intv = interviewers_by_id.get(r["interviewer_id"])

        start_at = r.get("start_at")
        end_at = r.get("end_at")

        date_str = None
        time_str = None
        if isinstance(start_at, str):
            try:
                date_part, time_part = start_at.split("T", 1)
                date_str = date_part
                time_str = time_part[:5]
            except ValueError:
                date_str = start_at

        items.append(
            {
                "interview_id": r["interview_id"],
                "candidate_name": (res or {}).get("full_name"),
                "candidate_email": (res or {}).get("email"),
                "job_id": r.get("job_id"),
                "job_title": (job or {}).get("role"),
                "interviewer_id": r.get("interviewer_id"),
                "interviewer_name": (intv or {}).get("name"),
                "interviewer_email": (intv or {}).get("email"),
                "date": date_str or day.isoformat(),
                "time": time_str,
                "status": r.get("status"),
                "meet_link": r.get("google_meet_link"),
                "calendar_link": r.get("google_html_link"),
                "start_at": start_at,
                "end_at": end_at,
            }
        )

    return {
        "date": day.isoformat(),
        "count": len(items),
        "interviews": items,
    }

@app.get("/recruiter/interviews/{interview_id}/overview")
async def recruiter_interview_overview(
    interview_id: str,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Full 360° view for recruiter:
    - interview basic info
    - resume + JD scores
    - interviewer feedback
    - transcript + AI report (from meetings)
    - recruiter note
    """
    # 1) Load interview
    interview = _first_row(
        supabase.table("interviews")
        .select("*")
        .eq("interview_id", interview_id)
        .limit(1)
        .execute()
    )
    if not interview:
        raise HTTPException(404, "Interview not found")

    # 2) Ensure recruiter owns this job
    job = _get_job_owned(interview["job_id"], current_user)  # also returns the job row

    # 3) Resume
    resume = _first_row(
        supabase.table("resumes")
        .select(
            "resume_id, candidate_id, full_name, email, phone, location, "
            "ai_summary, jd_match_score, skill_match_score, experience_match_score, "
            "education_match_score, raw_text, skills, experience, education, projects, certifications"
        )
        .eq("resume_id", interview["resume_id"])
        .limit(1)
        .execute()
    )

    # 4) Candidate profile (optional)
    candidate = None
    if resume and resume.get("candidate_id"):
        candidate = _first_row(
            supabase.table("candidates")
            .select("candidate_id, first_name, last_name, full_name, email, phone, location, links")
            .eq("candidate_id", resume["candidate_id"])
            .limit(1)
            .execute()
        )

    # 5) Interviewer feedback
    feedback = _first_row(
        supabase.table("interview_feedback")
        .select("*")
        .eq("interview_id", interview_id)
        .limit(1)
        .execute()
    )

    # 6) Transcript + AI report (meetings table uses meeting_id == interview_id)
    meeting = _first_row(
        supabase.table("meetings")
        .select("transcript, ai_report")
        .eq("meeting_id", interview_id)
        .limit(1)
        .execute()
    ) or {}

    return {
        "interview": {
            "interview_id": interview["interview_id"],
            "status": interview.get("status"),
            "start_at": interview.get("start_at"),
            "end_at": interview.get("end_at"),
            "meet_link": interview.get("google_meet_link"),
            "calendar_link": interview.get("google_html_link"),
            "feedback_overall_score": interview.get("feedback_overall_score"),
            "recruiter_note": interview.get("recruiter_note"),
        },
        "job": {
            "job_id": job["job_id"],
            "role": job.get("role"),
            "location": job.get("location"),
            "employment_type": job.get("employment_type"),
            "work_mode": job.get("work_mode"),
            "jd_text": job.get("jd_text"),
        },
        "candidate": candidate,
        "resume": resume,
        "feedback": feedback,
        "transcript": meeting.get("transcript"),
        "ai_report": meeting.get("ai_report"),
    }

# ---------- company ----------
@app.post("/company/create")
async def create_company(request: CompanyRequest, current_user: UserIdentity = Depends(get_current_user)):
    try:
        existing_company = (
            supabase.table("companies").select("*").eq("company_name", request.company_name).execute()
        )

        if existing_company.data:
            company_id = existing_company.data[0]["company_id"]
        else:
            company_result = supabase.table("companies").insert(
                {
                    "company_name": request.company_name,
                    "location": request.location,
                    "linkedin_url": request.linkedin_url,
                    "description": request.description,
                }
            ).execute()
            company_id = company_result.data[0]["company_id"]

        recruiter_data = {
            "user_id": current_user.user_id,
            "company_email": current_user.email,
            "full_name": request.recruiter_name,
            "company_id": company_id,
        }

        existing_recruiter = (
            supabase.table("recruiters").select("*").eq("user_id", current_user.user_id).execute()
        )
        if existing_recruiter.data:
            recruiter_result = (
                supabase.table("recruiters")
                .update(recruiter_data)
                .eq("user_id", current_user.user_id)
                .execute()
            )
        else:
            recruiter_result = supabase.table("recruiters").insert(recruiter_data).execute()

        return {
            "message": "Company profile created successfully",
            "company_id": company_id,
            "recruiter": recruiter_result.data[0],
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error creating company: {e}")

# ---------- jd/create (UI flow) ----------
@app.post("/jd/create")
async def create_jd(request: JDCreateRequest, current_user: UserIdentity = Depends(get_current_user)):
    try:
        recruiter = (
            supabase.table("recruiters")
            .select("company_id")
            .eq("user_id", current_user.user_id)
            .execute()
        )

        if not recruiter.data:
            raise HTTPException(status_code=400, detail="Please create company profile first")

        company_id = recruiter.data[0]["company_id"]

        prompt = create_jd_prompt(request)
        jd_text = generate_jd_with_ollama(prompt)

        job_data = {
            "company_id": company_id,
            "created_by": current_user.user_id,
            "role": request.job_title,
            "location": request.location,
            "employment_type": request.employment_type,
            "work_mode": request.work_mode,
            "min_years": request.min_experience,
            "max_years": request.max_experience,
            "skills_must_have": json.dumps(request.skills_must_have),
            "skills_nice_to_have": json.dumps(request.skills_nice_to_have),
            "jd_text": jd_text,
            "status": "draft",
        }

        job_result = supabase.table("jobs").insert(job_data).execute()
        return {"message": "JD generated successfully", "job_id": job_result.data[0]["job_id"], "jd_text": jd_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating JD: {e}")

# ---------- jd/regenerate ----------
@app.post("/jd/regenerate/{job_id}")
async def regenerate_jd(job_id: str, current_user: UserIdentity = Depends(get_current_user)):
    try:
        job = (
            supabase.table("jobs")
            .select("*")
            .eq("job_id", job_id)
            .eq("created_by", current_user.user_id)
            .execute()
        )
        if not job.data:
            raise HTTPException(status_code=404, detail="Job not found")

        job_data = job.data[0]

        comp = (
            supabase.table("companies")
            .select("company_name, description")
            .eq("company_id", job_data["company_id"])
            .execute()
        )
        company_name = comp.data[0].get("company_name") if comp.data else ""
        company_desc = comp.data[0].get("description") if comp.data else None

        req_obj = JDCreateRequest(
            job_title=job_data["role"],
            min_experience=job_data.get("min_years"),
            max_experience=job_data.get("max_years"),
            company_name=company_name,
            company_description=company_desc,
            employment_type=job_data["employment_type"],
            work_mode=job_data["work_mode"],
            skills_must_have=json.loads(job_data.get("skills_must_have", "[]")),
            skills_nice_to_have=json.loads(job_data.get("skills_nice_to_have", "[]")),
            requirements="Regenerate based on previous requirements",
            location=job_data.get("location"),
        )

        prompt = create_jd_prompt(req_obj)
        jd_text = generate_jd_with_ollama(prompt)

        supabase.table("jobs").update({"jd_text": jd_text}).eq("job_id", job_id).execute()

        return {"message": "JD regenerated successfully", "job_id": job_id, "jd_text": jd_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error regenerating JD: {e}")

# ---------- jd/edit ----------
@app.put("/jd/edit/{job_id}")
async def edit_jd(job_id: str, jd_text: str, current_user: UserIdentity = Depends(get_current_user)):
    try:
        result = (
            supabase.table("jobs")
            .update({"jd_text": jd_text})
            .eq("job_id", job_id)
            .eq("created_by", current_user.user_id)
            .execute()
        )
        if not result.data:
            raise HTTPException(status_code=404, detail="Job not found or unauthorized")
        return {"message": "JD updated successfully", "job_id": job_id, "jd_text": jd_text}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error updating JD: {e}")

# ---------- jobs/my-jobs ----------
@app.get("/jobs/my-jobs")
async def get_my_jobs(current_user: UserIdentity = Depends(get_current_user)):
    try:
        jobs = (
            supabase.table("jobs")
            .select("*")
            .eq("created_by", current_user.user_id)
            .order("created_at", desc=True)
            .execute()
        )
        return {"jobs": jobs.data}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error fetching jobs: {e}")

# ---------- NEW: recruiter pastes full JD text ----------
@app.post("/jd/ingest-text")
async def ingest_jd_text(
    payload: JDIngestText,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Recruiter pastes JD text.
    1. ensure user has recruiter/company
    2. parse JD text
    3. if missing fields -> return questions (no insert)
    4. otherwise -> insert into jobs as draft
    """
    # 1) ensure recruiter & company
    recruiter_resp = (
        supabase.table("recruiters")
        .select("company_id, full_name")
        .eq("user_id", current_user.user_id)
        .execute()
    )
    if not recruiter_resp.data:
        # recruiter hasn't created company yet
        return JSONResponse(
            status_code=200,
            content={
                "status": "needs_company",
                "message": "Please create your company profile first.",
            },
        )

    company_id = recruiter_resp.data[0]["company_id"]

    # 2) parse JD text
    parsed = _extract_fields_from_jd_text(payload.jd_text)

    # 3) detect missing
    questions = _detect_missing_fields(parsed)
    if questions:
        return JSONResponse(
            status_code=200,
            content={
                "status": "needs_input",
                "parsed": parsed,
                "questions": questions,
            },
        )

    # 4) normalize values so they pass DB constraints
    work_mode = normalize_work_mode(parsed.get("work_mode"))
    employment_type = normalize_employment_type(parsed.get("employment_type"))

    # 5) insert
    job_payload = {
        "company_id": company_id,
        "created_by": current_user.user_id,
        "role": parsed.get("job_title") or "Untitled Role",
        "location": parsed.get("location"),
        "employment_type": employment_type,
        "work_mode": work_mode,  # <--- guaranteed online/offline/hybrid now
        "min_years": parsed.get("min_experience"),
        "max_years": parsed.get("max_experience"),
        "skills_must_have": json.dumps(parsed.get("skills_must_have") or []),
        "skills_nice_to_have": json.dumps(parsed.get("skills_nice_to_have") or []),
        "jd_text": parsed.get("jd_text"),
        "status": "draft",
    }

    try:
        job_res = supabase.table("jobs").insert(job_payload).execute()
    except Exception as e:
        # If for some reason DB still rejects, tell frontend what we tried
        raise HTTPException(
            status_code=500,
            detail=f"Could not insert JD into jobs: {e}",
        )

    job_id = job_res.data[0]["job_id"]
    return {
        "status": "ok",
        "job_id": job_id,
        "job": job_res.data[0],
    }

@app.post("/jd/finalize/{job_id}")
async def finalize_jd(job_id: str, current_user: UserIdentity = Depends(get_current_user)):
    try:
        result = (
            supabase.table("jobs")
            .update({"status": "published"})
            .eq("job_id", job_id)
            .eq("created_by", current_user.user_id)
            .execute()
        )
        if not result.data:
            raise HTTPException(status_code=404, detail="Job not found or unauthorized")
        return {"message": "JD finalized and published", "job_id": job_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error finalizing JD: {e}")

@app.post("/jd/ingest-answer")
async def ingest_answer(
    payload: JDIngestAnswer,
    current_user: UserIdentity = Depends(get_current_user),
):
    # still need a company
    recruiter_resp = (
        supabase.table("recruiters")
        .select("company_id")
        .eq("user_id", current_user.user_id)
        .execute()
    )
    if not recruiter_resp.data:
        return {
            "status": "needs_company",
            "message": "Please create your company profile first.",
        }

    company_id = recruiter_resp.data[0]["company_id"]

    parsed = dict(payload.parsed or {})
    # merge answers
    for k, v in (payload.answers or {}).items():
        parsed[k] = v

    # re-check missing
    missing = _detect_missing_fields(parsed)
    if missing:
        return {
            "status": "needs_input",
            "parsed": parsed,
            "questions": missing,
        }

    # normalize
    work_mode = normalize_work_mode(parsed.get("work_mode"))
    employment_type = normalize_employment_type(parsed.get("employment_type"))

    job_payload = {
        "company_id": company_id,
        "created_by": current_user.user_id,
        "role": parsed.get("job_title") or "Untitled Role",
        "location": parsed.get("location"),
        "employment_type": employment_type,
        "work_mode": work_mode,
        "min_years": parsed.get("min_experience"),
        "max_years": parsed.get("max_experience"),
        "skills_must_have": json.dumps(parsed.get("skills_must_have") or []),
        "skills_nice_to_have": json.dumps(parsed.get("skills_nice_to_have") or []),
        "jd_text": parsed.get("jd_text") or payload.original_jd_text,
        "status": "draft",
    }

    job_res = supabase.table("jobs").insert(job_payload).execute()
    return {
        "status": "ok",
        "job_id": job_res.data[0]["job_id"],
        "job": job_res.data[0],
    }

# ---------------------------------------------------------------------------
# ----------------------- NEW: RESUME PIPELINE ------------------------------
# ---------------------------------------------------------------------------

def _sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def _extract_text_pdf(pdf_bytes: bytes) -> str:
    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
        return "\n".join([p.get_text("text") for p in doc])

def _norm_token(s: str) -> str:
    return re.sub(r"[^a-z0-9+\-.#]", "", (s or "").lower())

def _canonicalize(sk: str) -> str:
    m = {
        "reactjs": "react", "react.js": "react",
        "nodejs": "node", "node.js": "node",
        "ts": "typescript", "py": "python",
        "c++": "cpp", "c#": "csharp",
    }
    x = _norm_token(sk)
    return m.get(x, x)

def _skill_groups(must: List[str], nice: List[str]) -> Dict[str, set]:
    groups: Dict[str, set] = {}
    for s in (must or []) + (nice or []):
        if not s:
            continue
        can = _canonicalize(s)
        if not can:
            continue
        groups.setdefault(can, set()).add(can)
    if "javascript" in groups:
        groups["javascript"].update({"js"})
    if "html" in groups:
        groups["html"].update({"html5"})
    if "css" in groups:
        groups["css"].update({"css3"})
    return groups

def _flatten_resume_tokens(text: str) -> set:
    toks = {_canonicalize(t) for t in re.findall(r"[A-Za-z0-9+.#\-]{2,}", text or "")}
    return {t for t in toks if t}

def _years_from_text(text: str) -> float:
    y = 0.0
    for m in re.finditer(r"(\d{4})\s*[-–]\s*(\d{4}|present|current)", (text or "").lower()):
        a, b = m.group(1), m.group(2)
        try:
            aa = int(a)
            bb = datetime.datetime.utcnow().year if ("present" in b or "current" in b) else int(b)
            if bb >= aa:
                y += (bb - aa)
        except Exception:
            pass
    return round(y, 2)

def _experience_match(actual: float, miny: Optional[float], maxy: Optional[float]) -> int:
    if actual is None:
        return 0
    if miny is None and maxy is None:
        return 0
    if miny is not None and actual < miny:
        return max(0, round(100 * (actual / max(miny, 0.01))))
    return 100

def _score(text: str, must: List[str], nice: List[str], miny: Optional[float], maxy: Optional[float]) -> Dict[str, Any]:
    groups = _skill_groups(must, nice)
    res_tokens = _flatten_resume_tokens(text)
    matched = [g for g, aliases in groups.items() if res_tokens & aliases]
    total_groups = max(1, len(groups))
    skill_match = round(100.0 * len(matched) / total_groups, 2)
    yoe = _years_from_text(text)
    exp_score = _experience_match(yoe, miny, maxy)
    jd_match = round(0.65 * skill_match + 0.35 * exp_score, 2)
    return {
        "jd_match_score": jd_match,
        "meta": {
            "total_experience_years": yoe,
            "skill_groups_total": total_groups,
            "skill_groups_matched": matched,
            "skill_match_score": skill_match,
            "experience_match_score": exp_score,
            "top_matched_skills": matched[:10],
        }
    }
# ====== LLM helpers for resume parsing & summary (ADD THIS BLOCK) ======

# We’ll just reuse your Ollama caller.
def _ollama_generate(prompt: str) -> str:
    return generate_jd_with_ollama(prompt)

# Prompts

# --- JD parsing via AI (LLM) ---

JD_PARSE_PROMPT = """
You are a job description (JD) parsing assistant.

Return ONLY a JSON object with EXACTLY these top-level keys:
- job_title          (string or null)
- company_name       (string or null)
- location           (string or null)
- employment_type    (string or null, e.g. "Full-time", "Part-time", "Contract", "Internship")
- work_mode          (string or null, e.g. "remote", "onsite", "hybrid")
- min_experience     (number of years, float or null)
- max_experience     (number of years, float or null)
- skills_must_have   (array of strings)
- skills_nice_to_have(array of strings)
- requirements       (string or null; free text of main requirements / responsibilities)

Rules:
- If the JD does not clearly specify something, set that field to null instead of guessing.
- skills_must_have: technologies / skills that appear as mandatory, required, or core.
- skills_nice_to_have: technologies / skills marked as "nice to have", "good to have", or optional.
- min_experience / max_experience:
    - If range given like "3-5 years", min_experience=3, max_experience=5.
    - If "3+ years", min_experience=3, max_experience=null.
    - If not specified, use null for both.
- work_mode:
    - "remote" / "onsite" / "hybrid" if explicitly stated.
    - Otherwise null.
- employment_type:
    - "Full-time", "Part-time", "Contract", or "Internship" if the JD clearly states that.
    - Otherwise null.

Do NOT include any comments or explanations. Return ONLY the JSON.

JD TEXT:
{jd_text}
""".strip()


def _ensure_list_of_str(value) -> list[str]:
    """
    Normalize skills lists from the model:
    - list → trim each string
    - string → split on commas / newlines / semicolons / bullets
    """
    if isinstance(value, list):
        return [str(x).strip() for x in value if str(x).strip()]
    if isinstance(value, str):
        parts = re.split(r"[,;/\n•\-]+", value)
        return [p.strip() for p in parts if p.strip()]
    return []

ALLOWED_RESUME_STATUSES = {
    "PENDING",
    "PARSED",
    "SHORTLISTED",
    "INTERVIEW_SCHEDULED",
    "INTERVIEW_DONE",
    "REVIEW",
    "SELECTED",
    "REJECTED",
    "HIRED",
    "ABSENT",
    "CANCELLED",
}

EXTRACT_PROMPT = (
    "You are a resume parsing assistant. Return ONLY JSON with these exact top-level keys: "
    "first_name, last_name, full_name, email, phone, location, links, skills, experience, education, projects, certifications.\n"
    "Formatting rules:\n"
    "- links: object; optional keys: linkedin, github, portfolio, other (strings).\n"
    "- skills: object; keys: languages, frameworks, databases, tools, soft_skills (arrays of strings).\n"
    "- experience: array of objects with keys: company, title, start_date, end_date, description, technologies (array).\n"
    "- education: array of objects with keys: degree, institution, start_year, end_year, score.\n"
    "- projects: array of objects with keys: name, description, technologies (array), link, impact.\n"
    "- certifications: array of strings.\n"
    "No comments, no trailing commas.\n"
    "RESUME TEXT:\n{resume_text}"
)
# --- Rich JD↔︎Resume analysis (markdown) ---
ANALYSIS_PROMPT = """
You are a technical recruiter assistant. Compare the JOB DESCRIPTION and the CANDIDATE RESUME and write a concise analysis in Markdown.

Return ONLY Markdown with these sections (use the exact headings):
### Verdict
(one of: Strong Fit, Fit, Borderline, Not a Fit) with a one-sentence rationale.

### Key Matches
- Map must-have requirements to concrete evidence from the resume (role, dates, tech).

### Gaps
- List missing or weak items; mark each as **major** or **minor**.

### Experience Check
- Years parsed: {yoe} vs JD min: {miny} / max: {maxy}. Brief comment.

### Risk Flags
- Any red flags (employment gaps, very short stints, mismatched titles, etc.). If none, write "None noted."

### Scores
- Skill match: {skill_score} / 100
- Experience match: {exp_score} / 100
- Overall JD match: {overall_score} / 100

### Recommendation
- One of: Proceed to phone screen / Proceed to technical screen / Hold for backup / Reject (with reason).

DATA:
JD TITLE: {role}
MUST-HAVES: {must}
NICE-TO-HAVES: {nice}

JOB DESCRIPTION (truncated):
{jd}

CANDIDATE (parsed & truncated):
{cand}

Only use information present above. Be specific, avoid fluff. Keep total under ~250 words.
""".strip()

def _update_resume_status(resume_id: Optional[str], new_status: str) -> None:
    """
    Safely update the status of a resume row.
    - No-op if resume_id is None.
    - Ignores statuses not in ALLOWED_RESUME_STATUSES (to avoid typos).
    """
    if not resume_id:
        return
    if new_status not in ALLOWED_RESUME_STATUSES:
        return

    try:
        supabase.table("resumes").update({"status": new_status}).eq(
            "resume_id", resume_id
        ).execute()
    except Exception as e:
        print(f"[resume_status] failed to update for {resume_id}: {e}")

def generate_resume_analysis(
    candidate: dict,
    meta: dict,
    jd_text: str,
    role: Optional[str],
    must: List[str],
    nice: List[str],
    miny: Optional[float],
    maxy: Optional[float],
) -> str:
    # compact the candidate payload to keep prompt small
    cand_compact = {
        "name": candidate.get("full_name")
                or f"{candidate.get('first_name','')} {candidate.get('last_name','')}".strip(),
        "location": candidate.get("location"),
        "links": candidate.get("links"),
        "skills": candidate.get("skills"),
        "experience": (candidate.get("experience") or [])[:5],
        "education": (candidate.get("education") or [])[:3],
        "projects": (candidate.get("projects") or [])[:3],
        "certifications": candidate.get("certifications") or [],
    }

    overall = round(
        0.65 * float(meta.get("skill_match_score", 0)) +
        0.35 * float(meta.get("experience_match_score", 0)), 2
    )

    prompt = ANALYSIS_PROMPT.format(
        role=role or "Software Engineer",
        must=", ".join(must or []),
        nice=", ".join(nice or []),
        jd=(jd_text or "")[:2000],
        cand=json.dumps(cand_compact, ensure_ascii=False)[:2500],
        yoe=meta.get("total_experience_years", "n/a"),
        miny=miny if miny is not None else "n/a",
        maxy=maxy if maxy is not None else "n/a",
        skill_score=meta.get("skill_match_score", 0),
        exp_score=meta.get("experience_match_score", 0),
        overall_score=overall,
    )
    try:
        return _ollama_generate(prompt).strip()[:3000]
    except Exception:
        return "Analysis unavailable."

# JSON tidying for slightly-invalid LLM JSON
def _json_fragment(s: str) -> str:
    start = s.find("{")
    end = s.rfind("}")
    if start != -1 and end != -1 and end > start:
        return s[start:end+1]
    return s

def _tidy_json(s: str) -> str:
    s = re.sub(r",\s*([}\]])", r"\1", s)
    s = re.sub(r"\bTrue\b", "true", s)
    s = re.sub(r"\bFalse\b", "false", s)
    s = re.sub(r"\bNone\b", "null", s)
    s = re.sub(r"[\x00-\x1F\x7F]", " ", s)
    return s

# Normalize skills dict to consistent arrays
_DEF_SK_KEYS = ["languages", "frameworks", "databases", "tools", "soft_skills"]

def _normalize_skills_block(sk: dict) -> dict:
    out = {k: [] for k in _DEF_SK_KEYS}
    if isinstance(sk, dict):
        for k in _DEF_SK_KEYS:
            v = sk.get(k)
            if isinstance(v, list):
                out[k] = [str(x).strip() for x in v if str(x).strip()]
            elif isinstance(v, str):
                out[k] = [x.strip() for x in re.split(r",|/|\n", v) if x.strip()]
    return out

def parse_resume_structured(resume_text: str) -> dict:
    """LLM-based extractor → dict. Safe against minor JSON formatting issues."""
    try:
        raw = _ollama_generate(EXTRACT_PROMPT.format(resume_text=resume_text[:8000]))
        frag = _json_fragment(raw)
        try:
            data = json.loads(frag)
        except Exception:
            data = json.loads(_tidy_json(frag))
    except Exception:
        data = {}

    # Ensure shapes
    data = data or {}
    data.setdefault("links", {})
    data.setdefault("skills", {})
    data.setdefault("experience", [])
    data.setdefault("education", [])
    data.setdefault("projects", [])
    data.setdefault("certifications", [])
    data["skills"] = _normalize_skills_block(data.get("skills") or {})
    return data

def _ollama_json(prompt: str) -> dict:
    """
    Call Ollama and best-effort parse JSON out of the response,
    using the same fragment/tidy helpers as the resume parser.
    """
    raw = _ollama_generate(prompt)
    frag = _json_fragment(raw)
    try:
        return json.loads(frag)
    except Exception:
        try:
            return json.loads(_tidy_json(frag))
        except Exception:
            # Fallback: surface raw text so the UI can still show something
            return {"raw_text": raw}

# ====== end LLM helpers block ======

def _safe_json_list(x) -> List[str]:
    if isinstance(x, list):
        return x
    if isinstance(x, str):
        try:
            return json.loads(x)
        except Exception:
            return []
    return []

def _ensure_bucket():
    # idempotent
    try:
        supabase.storage.create_bucket(RESUME_BUCKET, {"public": False})
    except Exception:
        # bucket may already exist
        pass

sb = supabase
def _upload_to_storage(job_id: str, sha: str, filename: str, content: bytes) -> str:
    """
    Upload bytes to Supabase Storage with the correct content-type.
    Works across storage3 versions (uses UploadFileOptions if available).
    """
    _ensure_bucket()  # just in case
    bucket = supabase.storage.from_(RESUME_BUCKET)

    safe_name = re.sub(r'[^A-Za-z0-9._-]+', '_', filename or 'resume.pdf')
    key = f"{job_id}/{sha}_{safe_name}"

    # Newer storage3 supports UploadFileOptions
    if UploadFileOptions is not None:
        opts = UploadFileOptions(
            content_type="application/pdf",  # IMPORTANT (avoids 'text/plain is not supported')
            cache_control="3600",
            upsert=True,
        )
        bucket.upload(key, content, file_options=opts)  # use keyword arg
    else:
        # Older storage3: pass a headers-like dict via file_options (must be lowercase strings)
        bucket.upload(
            key,
            content,
            file_options={
                "content-type": "application/pdf",
                "cache-control": "3600",
                "x-upsert": "true",
            },
        )

    return key

@app.post("/jobs/{job_id}/parse_upload")
async def parse_upload(
    job_id: str,
    body: ParseOneRequest,
    current_user: UserIdentity = Depends(get_current_user),
):
    job = _get_job_owned(job_id, current_user)

    if not (body.storage_key or body.file_hash):
        raise HTTPException(400, "Provide storage_key or file_hash")

    # resolve storage_key if only file_hash was given
    storage_key = body.storage_key
    filename_hint = None
    if not storage_key:
        row = (
            supabase.table("resume_uploads")
            .select("storage_key, filename")
            .eq("job_id", job_id)
            .eq("file_hash", body.file_hash)
            .limit(1)
            .execute()
            .data
        )
        if not row:
            raise HTTPException(404, "Upload not found for this file_hash")
        storage_key = row[0]["storage_key"]
        filename_hint = row[0].get("filename")

    try:
        data = _download_from_storage(storage_key)
        pdf_text = _extract_text_pdf(data)
    except Exception as e:
        # store error and return
        supabase.table("resume_uploads").update(
            {"last_error": f"download/parse failed: {e}"}
        ).eq("job_id", job_id).eq("storage_key", storage_key).execute()
        raise HTTPException(400, f"Download/parse failed: {e}")

    # infer email from filename if possible
    email_hint = body.email_hint
    if not email_hint:
        fn = filename_hint or storage_key.split("/", 1)[-1]
        m = re.search(r"([\w\.-]+@[\w\.-]+\.\w+)", fn or "")
        if m:
            email_hint = m.group(1)

    _insert_or_update_resume(job, pdf_text, email_hint)

    supabase.table("resume_uploads").update(
        {"parsed_at": datetime.datetime.utcnow().isoformat() + "Z", "last_error": None}
    ).eq("job_id", job_id).eq("storage_key", storage_key).execute()

    return {"status": "ok", "storage_key": storage_key}

@app.post("/jobs/{job_id}/parse_pending")
async def parse_pending(
    job_id: str,
    limit: int = Query(100, ge=1, le=500),
    current_user: UserIdentity = Depends(get_current_user),
):
    job = _get_job_owned(job_id, current_user)
    # fetch unparsed uploads
    pending = (
        supabase.table("resume_uploads")
        .select("storage_key, filename")
        .eq("job_id", job_id)
        .is_("parsed_at", "null")  # supabase-py supports .is_ for IS NULL
        .limit(limit)
        .execute()
        .data
        or []
    )

    parsed, errors = 0, []

    for row in pending:
        key = row["storage_key"]
        try:
            data = _download_from_storage(key)
            pdf_text = _extract_text_pdf(data)

            # filename-based email hint
            email_hint = None
            m = re.search(r"([\w\.-]+@[\w\.-]+\.\w+)", (row.get("filename") or key) )
            if m:
                email_hint = m.group(1)

            _insert_or_update_resume(job, pdf_text, email_hint)

            supabase.table("resume_uploads").update(
                {"parsed_at": datetime.datetime.utcnow().isoformat() + "Z", "last_error": None}
            ).eq("job_id", job_id).eq("storage_key", key).execute()
            parsed += 1

        except Exception as e:
            msg = f"{key}: {e}"
            errors.append(msg)
            supabase.table("resume_uploads").update(
                {"last_error": msg}
            ).eq("job_id", job_id).eq("storage_key", key).execute()

    return {"status": "ok", "parsed": parsed, "errors": errors, "remaining_estimate": max(0, len(pending) - parsed)}

def _insert_or_update_resume(job: dict, pdf_text: str, email_hint: Optional[str]):
    # ---- email (unchanged) ----
    m = re.search(r"[\w\.-]+@[\w\.-]+\.\w+", pdf_text or "")
    email = (email_hint or "").strip().lower() or (m.group(0).lower() if m else None)
    if not email:
        email = hashlib.sha1((pdf_text or "")[:200].encode("utf-8")).hexdigest()[:16] + "@noemail.local"

    # ---- skills arrays may be json strings in your DB – handle both (unchanged) ----
    must = _safe_json_list(job.get("skills_must_have"))
    nice = _safe_json_list(job.get("skills_nice_to_have"))

    # ---- keep your existing scoring exactly as-is ----
    scored = _score(pdf_text or "", must, nice, job.get("min_years"), job.get("max_years"))
    meta = scored["meta"]
    jd_match = scored["jd_match_score"]

    # ---- NEW: LLM parse for richer candidate/resume fields (not used for scoring) ----
    cand = parse_resume_structured(pdf_text)  # dict: names, links, skills, experience, etc.

    # ---- upsert candidate (email unique) with richer fields ----
    supabase.table("candidates").upsert(
        {
            "email": email,
            "first_name": cand.get("first_name"),
            "last_name": cand.get("last_name"),
            "full_name": cand.get("full_name"),
            "phone": cand.get("phone"),
            "location": cand.get("location"),
            "links": cand.get("links") or {},
        },
        on_conflict="email",
    ).execute()
    cand_row = supabase.table("candidates").select("candidate_id").eq("email", email).execute().data
    candidate_id = cand_row and cand_row[0].get("candidate_id")
    # gather job skill lists in Python form
    must = _safe_json_list(job.get("skills_must_have"))
    nice = _safe_json_list(job.get("skills_nice_to_have"))

    # ... after you compute meta/jd_match and cand ...
    ai_analysis = generate_resume_analysis(
        cand,
        meta,
        job.get("jd_text") or "",
        job.get("role"),
        must,
        nice,
        job.get("min_years"),
        job.get("max_years"),
    )

    # ---- upsert resume row (unique: job_id + email) ----
    supabase.table("resumes").upsert(
        {
            "job_id": job["job_id"],
            "candidate_id": candidate_id,
            "email": email,
            "status": "PARSED",

            # structured fields from parser
            "first_name": cand.get("first_name"),
            "last_name": cand.get("last_name"),
            "full_name": cand.get("full_name"),
            "phone": cand.get("phone"),
            "location": cand.get("location"),
            "links": cand.get("links") or {},
            "skills": cand.get("skills") or {},
            "experience": cand.get("experience") or [],
            "education": cand.get("education") or [],
            "projects": cand.get("projects") or [],
            "certifications": cand.get("certifications") or [],

            # keep your existing meta + scores
            "meta": meta,
            "raw_text": pdf_text,
            "role": job.get("role"),
            "ai_summary": ai_analysis,
            "jd_match_score": jd_match,
            "skill_match_score": meta.get("skill_match_score"),
            "experience_match_score": meta.get("experience_match_score"),
            "education_match_score": meta.get("education_score", 0),
        },
        on_conflict="job_id,email",
    ).execute()

def _ensure_company(user: UserIdentity) -> Dict[str, Any]:
    """
    Ensures that the current authenticated user has a recruiter/company row.
    Returns the recruiter row (at least containing company_id), or 400 otherwise.
    """
    rec = _first_row(
        supabase.table("recruiters")
        .select("company_id")
        .eq("user_id", user.user_id)
        .limit(1)
        .execute()
    )

    if not rec:
        raise HTTPException(400, "Please create company profile first")

    return rec

def parse_jd_with_ai(jd_text: str) -> Dict[str, Any]:
    """
    Best-effort JD parser using the LLM.
    Returns a dict with the expected keys, or {} if anything goes wrong.
    """
    from fastapi import HTTPException  # already imported at top, but safe

    if not jd_text or not jd_text.strip():
        return {}

    prompt = JD_PARSE_PROMPT.format(jd_text=jd_text[:8000])

    try:
        data = _ollama_json(prompt)  # uses your existing Ollama JSON helper
    except HTTPException:
        return {}
    except Exception:
        return {}

    if not isinstance(data, dict):
        return {}

    # Ensure all expected keys exist with safe defaults
    job_title       = (data.get("job_title") or "").strip() or None
    company_name    = (data.get("company_name") or "").strip() or None
    location        = (data.get("location") or "").strip() or None
    employment_type = (data.get("employment_type") or "").strip() or None
    work_mode       = (data.get("work_mode") or "").strip() or None

    def _to_float_or_none(x):
        try:
            if x is None:
                return None
            f = float(x)
            return f
        except Exception:
            return None

    min_experience = _to_float_or_none(data.get("min_experience"))
    max_experience = _to_float_or_none(data.get("max_experience"))

    skills_must = _ensure_list_of_str(data.get("skills_must_have"))
    skills_nice = _ensure_list_of_str(data.get("skills_nice_to_have"))

    requirements = (data.get("requirements") or "").strip() or None

    return {
        "job_title": job_title,
        "company_name": company_name,
        "location": location,
        "employment_type": employment_type,
        "work_mode": work_mode,
        "min_experience": min_experience,
        "max_experience": max_experience,
        "skills_must_have": skills_must,
        "skills_nice_to_have": skills_nice,
        "requirements": requirements,
    }

@app.get("/jobs/{job_id}/ranked")
async def ranked(job_id: str, limit: int = Query(50, ge=1, le=200), current_user: UserIdentity = Depends(get_current_user)):
    _ = _get_job_owned(job_id, current_user)  # verify ownership
    rows = (
        supabase.table("resumes")
        .select("*")
        .eq("job_id", job_id)
        .order("jd_match_score", desc=True)
        .limit(limit)
        .execute()
        .data
    )
    return {"job_id": job_id, "count": len(rows), "rows": rows}

@app.get("/recruiter/jobs/{job_id}/candidates-ranked")
async def recruiter_candidates_ranked(
    job_id: str,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    For a given job, return ONLY candidates who have an interview with
    status in {INTERVIEW_SCHEDULED, INTERVIEW_DONE, ABSENT}, ranked by
    combined score: JD match + interviewer feedback + AI report.
    """
    # Ensure this recruiter owns the job
    job = _get_job_owned(job_id, current_user)

    # 1) All resumes for this job
    resumes = (
        supabase.table("resumes")
        .select(
            "resume_id, candidate_id, full_name, email, jd_match_score, "
            "skill_match_score, experience_match_score"
        )
        .eq("job_id", job_id)
        .execute()
        .data
        or []
    )
    if not resumes:
        return {"job_id": job_id, "role": job.get("role"), "candidates": []}

    resume_ids = [r["resume_id"] for r in resumes]

    # Only consider these interview statuses
    RANKABLE_STATUSES = [
        "INTERVIEW_SCHEDULED",
        "INTERVIEW_DONE",
        "ABSENT",
        "REVIEW",
        "SELECTED",
        "REJECTED",
        "HIRED",
    ]

    # 2) Interviews for those resumes, filtered by status
    interviews = (
        supabase.table("interviews")
        .select("interview_id, resume_id, status, feedback_overall_score")
        .in_("resume_id", resume_ids)
        .in_("status", RANKABLE_STATUSES)
        .execute()
        .data
        or []
    )
    if not interviews:
        # No interviews in the desired statuses
        return {"job_id": job_id, "role": job.get("role"), "candidates": []}

    interviews_by_resume = {i["resume_id"]: i for i in interviews}
    interview_ids = [i["interview_id"] for i in interviews]

    # 3) Feedback rows
    feedback_rows = (
        supabase.table("interview_feedback")
        .select("interview_id, overall_score")
        .in_("interview_id", interview_ids)
        .execute()
        .data
        or []
    )
    fb_by_interview = {f["interview_id"]: f for f in feedback_rows}

    # 4) AI reports
    meetings = (
        supabase.table("meetings")
        .select("meeting_id, ai_report")
        .in_("meeting_id", interview_ids)
        .execute()
        .data
        or []
    )
    ai_by_meeting = {m["meeting_id"]: m for m in meetings}

    items = []
    for r in resumes:
        it = interviews_by_resume.get(r["resume_id"])
        if not it:
            # This resume either has no interview OR only interviews with
            # statuses outside RANKABLE_STATUSES -> skip it.
            continue

        fb_row = fb_by_interview.get(it["interview_id"])
        feedback_overall = (
            (fb_row or {}).get("overall_score")
            or it.get("feedback_overall_score")
        )

        mt = ai_by_meeting.get(it["interview_id"])
        ai_overall = None
        if mt and mt.get("ai_report"):
            ai_overall = (mt["ai_report"].get("analytics") or {}).get("overall_score")

        final_score_val = final_candidate_score(
            jd_match=r.get("jd_match_score"),
            feedback_overall=feedback_overall,
            ai_overall=ai_overall,
        )

        items.append(
            {
                "resume_id": r["resume_id"],
                "candidate_id": r.get("candidate_id"),
                "full_name": r.get("full_name"),
                "email": r.get("email"),
                "jd_match_score": r.get("jd_match_score"),
                "feedback_overall_score": feedback_overall,
                "ai_overall_score": ai_overall,
                "interview_id": it["interview_id"],
                "interview_status": it.get("status"),
                "final_score": final_score_val,
            }
        )

    # Sort best → worst
    items.sort(key=lambda x: x["final_score"], reverse=True)

    return {
        "job_id": job_id,
        "role": job.get("role"),
        "count": len(items),
        "candidates": items,
    }

@app.post("/jobs/{job_id}/rescore_existing")
async def rescore_existing(job_id: str, current_user: UserIdentity = Depends(get_current_user)):
    job = _get_job_owned(job_id, current_user)
    must = _safe_json_list(job.get("skills_must_have"))
    nice = _safe_json_list(job.get("skills_nice_to_have"))

    res = (
        supabase.table("resumes")
        .select("resume_id, raw_text")
        .eq("job_id", job_id)
        .execute()
        .data
        or []
    )
    updated = 0
    for r in res:
        scored = _score(r.get("raw_text") or "", must, nice, job.get("min_years"), job.get("max_years"))
        meta = scored["meta"]; jd_match = scored["jd_match_score"]

        # reparse candidate (small cost, keeps analysis fresh)
        cand = parse_resume_structured(r.get("raw_text") or "")
        ai_analysis = generate_resume_analysis(
            cand,
            meta,
            job.get("jd_text") or "",
            job.get("role"),
            must,
            nice,
            job.get("min_years"),
            job.get("max_years"),
        )

        supabase.table("resumes").update(
            {
                "jd_match_score": jd_match,
                "skill_match_score": meta.get("skill_match_score"),
                "experience_match_score": meta.get("experience_match_score"),
                "meta": meta,
                "ai_summary": ai_analysis,   # keep UI up to date
            }
        ).eq("resume_id", r["resume_id"]).execute()
        updated += 1

    return {"status": "ok", "rescored": updated}

@app.post("/storage/cleanup")
async def cleanup(current_user: UserIdentity = Depends(get_current_user)):
    now = datetime.datetime.utcnow().isoformat() + "Z"
    expired = supabase.table("resume_uploads").select("*").lt("expires_at", now).execute().data or []
    bucket = supabase.storage.from_(RESUME_BUCKET)
    deleted = 0
    for row in expired:
        try:
            bucket.remove([row["storage_key"]])
        except Exception:
            pass
        supabase.table("resume_uploads").delete().eq("job_id", row["job_id"]).eq("file_hash", row["file_hash"]).execute()
        deleted += 1
    return {"deleted": deleted}

@app.post("/jobs/{job_id}/upload_resumes")
async def upload_resumes(
    job_id: str,
    files: List[UploadFile] = File(...),
    current_user: UserIdentity = Depends(get_current_user),
):
    job = _get_job_owned(job_id, current_user)

    if not files:
        raise HTTPException(400, "No files provided")

    results = []
    for f in files:
        try:
            data = await f.read()
            if not data:
                results.append({"file_name": f.filename, "status": "skipped_empty"})
                continue

            ct = (f.content_type or "").lower()
            is_pdf_ct = ct in ("application/pdf", "application/octet-stream", "binary/octet-stream")
            is_pdf_magic = data[:5] == b"%PDF-"
            if not (is_pdf_ct or is_pdf_magic):
                results.append({
                    "file_name": f.filename,
                    "status": "skipped_invalid_type",
                    "reason": f"Not a PDF (content_type={f.content_type})"
                })
                continue

            sha = _sha256_bytes(data)
            exists = (
                supabase.table("resume_uploads")
                .select("storage_key")
                .eq("job_id", job_id)
                .eq("file_hash", sha)
                .execute()
                .data
            )
            if exists:
                results.append({
                    "file_name": f.filename,
                    "file_hash": sha,
                    "storage_key": exists[0]["storage_key"],
                    "status": "duplicate"
                })
                continue

            storage_key = _upload_to_storage(job_id, sha, f.filename or "resume.pdf", data)
            expires_at = (datetime.datetime.utcnow() + datetime.timedelta(hours=RESUME_TTL_HOURS)).isoformat() + "Z"

            supabase.table("resume_uploads").upsert(
                {
                    "job_id": job_id,
                    "file_hash": sha,
                    "storage_key": storage_key,
                    "uploader_id": current_user.user_id,
                    "expires_at": expires_at,
                    "parsed_at": None,
                    "filename": f.filename,
                    "content_type": f.content_type,
                    "last_error": None,
                },
                on_conflict="job_id,file_hash",
            ).execute()

            results.append({"file_name": f.filename, "file_hash": sha, "storage_key": storage_key, "status": "ok"})

        except Exception as e:
            results.append({"file_name": getattr(f, "filename", None), "status": "error", "reason": str(e)})

    return {"status": "ok", "count": len(results), "results": results}

@app.get("/jobs/{job_id}/resumes")
async def list_resumes(
    job_id: str,
    status: Optional[str] = Query(None, description="Filter by status: PENDING | PARSED | REJECTED"),
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    List resumes for a job with optional status filter and simple pagination.
    - Sorts by jd_match_score DESC, created_at DESC (if present).
    - Returns total count (for client-side paging) and next_offset cursor.
    """
    # Ownership check
    _ = _get_job_owned(job_id, current_user)

    # Validate status (if provided)
    if status is not None and status not in ALLOWED_RESUME_STATUSES:
        raise HTTPException(status_code=400, detail=f"Invalid status. Use one of {sorted(ALLOWED_RESUME_STATUSES)}")

    q = (
        supabase.table("resumes")
        .select("*", count="exact")
        .eq("job_id", job_id)
    )
    if status:
        q = q.eq("status", status)

    # Primary sort by match score, fallback by created_at if you have that column
    # Supabase allows multiple .order(...) calls
    q = q.order("jd_match_score", desc=True)
    try:
        q = q.order("created_at", desc=True)  # safe if column exists; otherwise ignore/remove
    except Exception:
        pass

    # Pagination
    # Supabase "range" is inclusive on both ends
    end = offset + limit - 1
    q = q.range(offset, end)

    resp = q.execute()
    rows = resp.data or []
    total = getattr(resp, "count", None)

    # next_offset-style cursor
    next_offset = None
    if len(rows) == limit:
        next_offset = offset + limit

    return {
        "job_id": job_id,
        "status_filter": status or "ANY",
        "total": total,           # may be None on older libs; if so, omit in UI
        "limit": limit,
        "offset": offset,
        "next_offset": next_offset,
        "rows": rows,
    }

# ===================== INTERVIEWERS CRUD =====================

# Create
@app.post("/interviewers", response_model=InterviewerOut)
async def invite_interviewer(
    body: InterviewerInvite,
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    # enforce uniqueness per company
    existing = (
        supabase.table("interviewers")
        .select("interviewer_id")
        .eq("company_id", company_id)
        .eq("email", body.email.lower())
        .limit(1)
        .execute()
        .data
    )
    if existing:
        raise HTTPException(409, "Interviewer with this email already exists in this company")

    admin = admin_sb.auth.admin

    # 1) Send an invite email (Supabase sends it via SMTP)
    # NOTE: 'options' shape varies with library versions, we handle both.
    try:
        invited = admin.invite_user_by_email(
            body.email.lower(),
            { "redirect_to": INVITE_REDIRECT }
        )
    except Exception:
        # older signatures sometimes use 'data' kw; this keeps it robust
        invited = admin.invite_user_by_email(body.email.lower(), {"redirect_to": INVITE_REDIRECT})

    auth_user_id = invited.user.id

    # 2) Set metadata (invite API only sets user_metadata by default)
    admin.update_user_by_id(auth_user_id, {
        "app_metadata":  {"role": "interviewer", "company_id": company_id},
        "user_metadata": {"name": body.name},
    })

    # 3) Insert your shadow row
    row = (
        supabase.table("interviewers")
        .insert({
            "company_id": company_id,
            "auth_user_id": auth_user_id,
            "name": body.name.strip(),
            "email": body.email.lower(),
            "is_active": bool(body.is_active),
            "created_by": current_user.user_id,
        })
        .execute()
        .data[0]
    )

    return {
        "interviewer_id": row["interviewer_id"],
        "name": row["name"],
        "email": row["email"],
        "company_id": row.get("company_id"),
        "is_active": row.get("is_active", True),
    }

@app.get("/interviewers", response_model=List[InterviewerOut])
async def list_interviewers(
    q: Optional[str] = Query(None, description="search by name/email"),
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
    current_user: UserIdentity = Depends(get_current_user),
):
    company_id = _get_company_id_for_user(current_user)
    if not company_id:
        raise HTTPException(400, "Please create company profile first")

    query = (
        supabase.table("interviewers")
        .select("interviewer_id, name, email, company_id, is_active")
        .eq("company_id", company_id)
    )

    # Optional order if column exists
    try:
        query = query.order("created_at", desc=True)
    except Exception:
        pass

    if q:
        # Search both name and email (PostgREST OR syntax)
        try:
            query = query.or_(f"email.ilike.%{q}%,name.ilike.%{q}%")
        except Exception:
            # Minimal fallback: filter by email only
            query = query.filter("email", "ilike", f"%{q}%")

    end = offset + limit - 1
    rows = query.range(offset, end).execute().data or []
    return [
        {
            "interviewer_id": r["interviewer_id"],
            "name": r["name"],
            "email": r["email"],
            "company_id": r.get("company_id"),
            "is_active": r.get("is_active", True),
        }
        for r in rows
    ]

# Read one
@app.get("/interviewers/{interviewer_id}", response_model=InterviewerOut)
async def get_interviewer(
    interviewer_id: UUID,
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    row = _first_row(
        supabase.table("interviewers")
        .select("interviewer_id, name, email, company_id, is_active")
        .eq("interviewer_id", str(interviewer_id))
        .eq("company_id", company_id)
        .limit(1)
        .execute()
    )

    if not row:
        raise HTTPException(404, "Interviewer not found")

    return row

# Update
@app.patch("/interviewers/{interviewer_id}", response_model=InterviewerOut)
async def update_interviewer(
    interviewer_id: UUID,
    body: InterviewerUpdate,
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    existing = _first_row(
        supabase.table("interviewers")
        .select("interviewer_id, auth_user_id, name, email, company_id, is_active")
        .eq("interviewer_id", str(interviewer_id))
        .eq("company_id", company_id)
        .limit(1)
        .execute()
    )

    if not existing:
        raise HTTPException(404, "Interviewer not found")

    update_data = body.dict(exclude_unset=True)

    if "email" in update_data and update_data["email"]:
        update_data["email"] = update_data["email"].lower()

    # Sync email to auth user if there is a linked auth_user_id
    if existing.get("auth_user_id") and "email" in update_data:
        try:
            admin_sb.auth.admin.update_user_by_id(
                existing["auth_user_id"],
                {"email": update_data["email"]},
            )
        except Exception as e:
            raise HTTPException(
                500, f"Failed to update auth user email: {e}"
            )

    if not update_data:
        # Nothing to update; just return the existing record
        return existing

    updated_rows = (
        supabase.table("interviewers")
        .update(update_data)
        .eq("interviewer_id", str(interviewer_id))
        .eq("company_id", company_id)
        .execute()
        .data
    )

    # Supabase update returns a list
    return updated_rows[0] if updated_rows else existing

@app.post("/interviewers/{interviewer_id}/deactivate")
async def deactivate_interviewer(
    interviewer_id: str,
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    row = (
        supabase.table("interviewers")
        .select("auth_user_id")
        .eq("interviewer_id", interviewer_id)
        .eq("company_id", company_id)
        .single()
        .execute()
        .data
    )
    if not row:
        raise HTTPException(404, "Interviewer not found")

    admin = admin_sb.auth.admin
    # Mark blocked in app_metadata (enforce in RLS or app logic)
    admin.update_user_by_id(row["auth_user_id"], {"app_metadata": {"blocked": True}})
    supabase.table("interviewers").update({"is_active": False}).eq("interviewer_id", interviewer_id).execute()
    return {"message": "Interviewer deactivated"}

@app.post("/interviewers/{interviewer_id}/reactivate")
async def reactivate_interviewer(
    interviewer_id: str,
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    row = (
        supabase.table("interviewers")
        .select("auth_user_id")
        .eq("interviewer_id", interviewer_id)
        .eq("company_id", company_id)
        .single()
        .execute()
        .data
    )
    if not row:
        raise HTTPException(404, "Interviewer not found")

    admin = admin_sb.auth.admin
    admin.update_user_by_id(row["auth_user_id"], {"app_metadata": {"blocked": False}})
    supabase.table("interviewers").update({"is_active": True}).eq("interviewer_id", interviewer_id).execute()
    return {"message": "Interviewer reactivated"}

@app.delete("/interviewers/{interviewer_id}")
async def delete_interviewer(
    interviewer_id: str,
    hard: bool = Query(False, description="set true to hard delete"),
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    row = (
        supabase.table("interviewers")
        .select("auth_user_id")
        .eq("interviewer_id", interviewer_id)
        .eq("company_id", company_id)
        .single()
        .execute()
        .data
    )
    if not row:
        raise HTTPException(404, "Interviewer not found")

    if hard:
        supabase.auth.admin.delete_user(row["auth_user_id"])
        supabase.table("interviewers").delete().eq("interviewer_id", interviewer_id).eq("company_id", company_id).execute()
        return {"message": "Interviewer deleted", "hard": True}

    # soft delete
    supabase.table("interviewers").update({"is_active": False}).eq("interviewer_id", interviewer_id).eq("company_id", company_id).execute()
    supabase.auth.admin.update_user_by_id(row["auth_user_id"], {"app_metadata": {"blocked": True}})
    return {"message": "Interviewer deactivated", "hard": False}

@app.post("/interviewers/{interviewer_id}/send-reset", response_model=SendResetOut)
async def send_reset_link(
    interviewer_id: UUID,
    current_user: UserIdentity = Depends(get_current_user),
):
    rec = _ensure_company(current_user)
    company_id = rec["company_id"]

    row = (
        supabase.table("interviewers")
        .select("email")
        .eq("interviewer_id", str(interviewer_id))
        .eq("company_id", company_id)
        .single()
        .execute()
        .data
    )
    if not row:
        raise HTTPException(404, "Interviewer not found")

    email = row["email"].lower()
    _send_supabase_password_reset(email)

    return {"action_link": f"(email sent) redirect_to={RESET_REDIRECT}"}

@app.post("/auth/interviewer/signin", response_model=InterviewerSessionOut)
async def interviewer_signin(body: InterviewerSignIn):
    try:
        # 1) sign in with email/password
        resp = public_sb.auth.sign_in_with_password({"email": body.email, "password": body.password})
        if not resp.session or not resp.user:
            raise HTTPException(status_code=401, detail="Invalid credentials")

        user = resp.user
        # 2) optional safety: honor app_metadata.blocked
        app_meta = (getattr(user, "app_metadata", None) or {})
        if app_meta.get("blocked") is True:
            raise HTTPException(status_code=403, detail="Account is blocked")

        # 3) find the interviewer shadow row by auth_user_id (preferred), fallback by email
        row = (
            supabase.table("interviewers")
            .select("*")
            .eq("auth_user_id", user.id)
            .single()
            .execute()
            .data
        )
        if not row:
            row = (
                supabase.table("interviewers")
                .select("*")
                .eq("email", body.email.lower())
                .single()
                .execute()
                .data
            )

        if not row:
            raise HTTPException(status_code=403, detail="Not an interviewer")
        if not row.get("is_active", True):
            raise HTTPException(status_code=403, detail="Interviewer is deactivated")

        return {
            "access_token": resp.session.access_token,
            "refresh_token": resp.session.refresh_token,
            "user": {"id": user.id, "email": user.email},
            "interviewer": {
                "interviewer_id": row["interviewer_id"],
                "name": row.get("name"),
                "email": row.get("email"),
                "company_id": row.get("company_id"),
                "is_active": row.get("is_active", True),
            },
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=401, detail=f"Login failed: {e}")

@app.get("/interviewer/me")
async def interviewer_me(current_user: UserIdentity = Depends(get_current_user)):
    resp = (
        supabase.table("interviewers")
        .select("interviewer_id, name, email, company_id, is_active")
        .eq("auth_user_id", current_user.user_id)
        .limit(1)
        .execute()
    )
    row = _first_row(resp)
    if not row:
        raise HTTPException(status_code=404, detail="Interviewer profile not found")
    if not row.get("is_active", True):
        raise HTTPException(status_code=403, detail="Interviewer is deactivated")
    return {
        "user": {"id": current_user.user_id, "email": current_user.email},
        "interviewer": row,
    }

@app.get("/interviewer/interviews/today")
async def list_my_interviews_today(
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    For the logged-in interviewer, list all INTERVIEW_SCHEDULED interviews for *today*
    (using Asia/Kolkata as the logical day).
    Returns: candidate name, job title, date, time, status, Google Meet link, etc.
    """
    # 1) Find interviewer row from auth_user_id
    interviewer = (
        supabase.table("interviewers")
        .select("interviewer_id, name, email, company_id, is_active")
        .eq("auth_user_id", current_user.user_id)
        .single()
        .execute()
        .data
    )
    if not interviewer:
        raise HTTPException(status_code=404, detail="Interviewer profile not found")
    if not interviewer.get("is_active", True):
        raise HTTPException(status_code=403, detail="Interviewer is deactivated")

    interviewer_id = interviewer["interviewer_id"]

    # 2) Today's window in IST, converted to UTC for DB filtering
    day, start_iso, end_iso = _today_bounds_for_tz("Asia/Kolkata")

    # 3) Fetch today's scheduled interviews for this interviewer
    rows = (
        supabase.table("interviews")
        .select(
            "interview_id, job_id, resume_id, status, "
            "start_at, end_at, google_meet_link, google_html_link"
        )
        .eq("interviewer_id", interviewer_id)
        .eq("status", "INTERVIEW_SCHEDULED")
        .gte("start_at", start_iso)
        .lt("start_at", end_iso)
        .order("start_at", desc=False)
        .execute()
        .data
        or []
    )

    # 4) Bulk fetch jobs + resumes for names/titles
    job_ids = sorted({r["job_id"] for r in rows if r.get("job_id")})
    resume_ids = sorted({r["resume_id"] for r in rows if r.get("resume_id")})

    jobs_by_id: Dict[str, Dict[str, Any]] = {}
    resumes_by_id: Dict[str, Dict[str, Any]] = {}

    if job_ids:
        jobs = (
            supabase.table("jobs")
            .select("job_id, role")
            .in_("job_id", job_ids)
            .execute()
            .data
            or []
        )
        jobs_by_id = {j["job_id"]: j for j in jobs}

    if resume_ids:
        res_rows = (
            supabase.table("resumes")
            .select("resume_id, full_name, email")
            .in_("resume_id", resume_ids)
            .execute()
            .data
            or []
        )
        resumes_by_id = {r["resume_id"]: r for r in res_rows}

    items = []
    for r in rows:
        job = jobs_by_id.get(r["job_id"]) if r.get("job_id") else None
        res = resumes_by_id.get(r["resume_id"]) if r.get("resume_id") else None

        start_at = r.get("start_at")
        end_at = r.get("end_at")

        date_str = None
        time_str = None
        if isinstance(start_at, str):
            # naive string split: 2025-11-18T10:30:00+05:30
            try:
                date_part, time_part = start_at.split("T", 1)
                date_str = date_part
                time_str = time_part[:5]  # HH:MM
            except ValueError:
                date_str = start_at

        items.append(
            {
                "interview_id": r["interview_id"],
                "candidate_name": (res or {}).get("full_name"),
                "candidate_email": (res or {}).get("email"),
                "job_title": (job or {}).get("role"),
                "date": date_str or day.isoformat(),
                "time": time_str,
                "status": r.get("status"),
                "meet_link": r.get("google_meet_link"),
                "calendar_link": r.get("google_html_link"),
                "start_at": start_at,
                "end_at": end_at,
            }
        )

    return {
        "date": day.isoformat(),
        "count": len(items),
        "interviewer": {
            "interviewer_id": interviewer_id,
            "name": interviewer.get("name"),
            "email": interviewer.get("email"),
        },
        "interviews": items,
    }

@app.get("/interviewer/interviews")
async def list_my_interviews(
    status: Optional[str] = Query(
        None,
        description="Optional: filter by interview status (e.g. INTERVIEW_SCHEDULED, INTERVIEW_DONE, etc.)"
    ),
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    List all interviews assigned to the logged-in interviewer.
    Uses 'interviews' table and joins job + resume info in Python.
    """
    interviewer = _get_interviewer_by_auth_user(current_user)
    interviewer_id = interviewer["interviewer_id"]

    # Base query: all interviews for this interviewer
    q = (
        supabase.table("interviews")
        .select("interview_id, job_id, resume_id, status, start_at, end_at, google_meet_link, google_html_link")
        .eq("interviewer_id", interviewer_id)
    )
    if status:
        q = q.eq("status", status)

    # Sort by start time (soonest first)
    try:
        q = q.order("start_at", desc=False)
    except Exception:
        pass

    interviews = q.execute().data or []

    # ---- Bulk-load jobs + resumes to avoid N+1 queries ----
    job_ids = sorted({i["job_id"] for i in interviews if i.get("job_id")})
    resume_ids = sorted({i["resume_id"] for i in interviews if i.get("resume_id")})

    jobs_by_id: Dict[str, Dict[str, Any]] = {}
    resumes_by_id: Dict[str, Dict[str, Any]] = {}

    if job_ids:
        jobs = (
            supabase.table("jobs")
            .select("job_id, role, location")
            .in_("job_id", job_ids)
            .execute()
            .data
            or []
        )
        jobs_by_id = {j["job_id"]: j for j in jobs}

    if resume_ids:
        res_rows = (
            supabase.table("resumes")
            .select("resume_id, full_name, email")
            .in_("resume_id", resume_ids)
            .execute()
            .data
            or []
        )
        resumes_by_id = {r["resume_id"]: r for r in res_rows}

    # Build response list
    items = []
    for row in interviews:
        job = jobs_by_id.get(row["job_id"]) if row.get("job_id") else None
        res = resumes_by_id.get(row["resume_id"]) if row.get("resume_id") else None
        items.append(
            {
                "interview_id": row["interview_id"],
                "job_id": row.get("job_id"),
                "resume_id": row.get("resume_id"),
                "status": row.get("status"),
                "start_at": row.get("start_at"),
                "end_at": row.get("end_at"),
                # meet + calendar link from interviews table
                "meet_link": row.get("google_meet_link"),
                "calendar_link": row.get("google_html_link"),
                # summary info for UI
                "job_role": (job or {}).get("role"),
                "job_location": (job or {}).get("location"),
                "candidate_name": (res or {}).get("full_name"),
                "candidate_email": (res or {}).get("email"),
            }
        )

    return {
        "interviewer": {
            "interviewer_id": interviewer_id,
            "name": interviewer.get("name"),
            "email": interviewer.get("email"),
        },
        "count": len(items),
        "interviews": items,
    }

@app.get("/interviewer/interviews/{interview_id}")
async def get_interview_detail(
    interview_id: str,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Detailed view for a single interview assigned to this interviewer.
    - Loads interview from interviews table
    - Loads JD from jobs table (via job_id)
    - Loads resume & AI summary from resumes table (via resume_id)
    - Optionally enriches with candidates table if linked
    """
    interviewer = _get_interviewer_by_auth_user(current_user)
    interviewer_id = interviewer["interviewer_id"]

    # Interview must belong to this interviewer
    interview = _first_row(
        supabase.table("interviews")
        .select("*")
        .eq("interview_id", interview_id)
        .eq("interviewer_id", interviewer_id)
        .limit(1)
        .execute()
    )
    if not interview:
        raise HTTPException(status_code=404, detail="Interview not found")

    # JD from jobs
    job = _first_row(
        supabase.table("jobs")
        .select("job_id, role, location, employment_type, work_mode, jd_text")
        .eq("job_id", interview["job_id"])
        .limit(1)
        .execute()
    )

    # Resume from resumes
    resume_row = _first_row(
        supabase.table("resumes")
        .select(
            "resume_id, candidate_id, full_name, email, phone, location, "
            "raw_text, ai_summary, skills, experience, education, projects, "
            "certifications, jd_match_score, skill_match_score, experience_match_score"
        )
        .eq("resume_id", interview["resume_id"])
        .limit(1)
        .execute()
    )

    # Optional candidate profile (if candidate_id present)
    candidate = None
    if resume_row and resume_row.get("candidate_id"):
        candidate = _first_row(
            supabase.table("candidates")
            .select("candidate_id, first_name, last_name, full_name, email, phone, location, links")
            .eq("candidate_id", resume_row["candidate_id"])
            .limit(1)
            .execute()
        )

    return {
        "interview": {
            "interview_id": interview["interview_id"],
            "status": interview.get("status"),
            "start_at": interview.get("start_at"),
            "end_at": interview.get("end_at"),
            "meet_link": interview.get("google_meet_link"),
            "calendar_link": interview.get("google_html_link"),
        },
        "job": job,              # includes jd_text
        "resume": resume_row,    # includes raw_text + ai_summary + structured fields
        "candidate": candidate,  # optional
    }

# ===================== INTERVIEW COPILOT =====================

# High-level instructions for the copilot behavior
COPILOT_SYSTEM_HINT = """
You are an AI interview copilot helping a human interviewer evaluate a candidate.
Be concise, structured, and practical. Avoid over-explaining theory.
Always bias towards concrete, scenario-based questions that reveal real experience.
""".strip()


def _meet_context_for_session(session_id: str) -> dict:
    """
    Load meeting + transcript context for the AI copilot.
    Returns: {meeting, turns, interviewer_name, candidate_name, jd, resume}
    """
    mtg = _sb_get_meeting(session_id) or {}
    turns = _recent_turns(session_id, limit=2000)

    return {
        "meeting": mtg,
        "turns": turns,
        "interviewer_name": mtg.get("interviewer_name") or "Interviewer",
        "candidate_name": mtg.get("candidate_name") or "Candidate",
        "jd": mtg.get("jd") or "",
        "resume": mtg.get("resume") or "",
    }

@app.post("/interviews/{interview_id}/feedback")
async def submit_interview_feedback(
    interview_id: str,
    body: InterviewFeedbackIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Called by the interviewer after the interview.
    Stores raw choices + numeric scores in interview_feedback,
    and updates interviews.feedback_overall_score for ranking.
    """
    # 1) Ensure this auth user is the assigned interviewer
    ctx = _get_interview_context(interview_id, current_user)
    interviewer = ctx["interviewer"]
    interviewer_id = interviewer["interviewer_id"]

    # 2) Compute scores 0–10
    scores = compute_feedback_scores(body)

    # 3) Upsert feedback row
    row = {
        "interview_id": interview_id,
        "interviewer_id": interviewer_id,
        "overall_recommendation": body.overall_recommendation,
        "skills_level": body.skills_level,
        "communication_level": body.communication_level,
        "jd_fit_level": body.jd_fit_level,
        "flags": body.flags,
        "comment": body.comment,
        "skills_score": scores["skills_score"],
        "communication_score": scores["communication_score"],
        "jd_fit_score": scores["jd_fit_score"],
        "overall_score": scores["overall_score"],
    }

    fb = (
        supabase.table("interview_feedback")
        .upsert(row, on_conflict="interview_id,interviewer_id")
        .execute()
        .data[0]
    )

    # 4) Also store overall_score on interviews row for quick sort
    try:
        supabase.table("interviews").update(
            {"feedback_overall_score": scores["overall_score"]}
        ).eq("interview_id", interview_id).execute()
    except Exception as e:
        print("[feedback] failed to update interviews.feedback_overall_score:", e)

    return {
        "interview_id": interview_id,
        "feedback": fb,
        "scores": scores,
    }

@app.post("/jobs/{job_id}/resumes/shortlist")
async def shortlist_resumes(
    job_id: str,
    payload: BulkShortlistRequest,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Bulk-shortlist candidates for a job.
    - Accepts either 'resume_ids' or 'emails' (or both).
    - Default behavior updates rows currently in status == PARSED.
    - Returns counts for transparency.
    """
    # Verify recruiter owns the job
    _ = _get_job_owned(job_id, current_user)

    ids = [i for i in (payload.resume_ids or []) if i]
    emails = [e.lower() for e in (payload.emails or []) if e]
    if not ids and not emails:
        raise HTTPException(status_code=400, detail="Provide resume_ids and/or emails")

    updated_total = 0
    skipped = 0
    errors: List[Dict[str, Any]] = []

    # What we will set
    new_status = "SHORTLISTED"
    if new_status not in ALLOWED_RESUME_STATUSES:
        raise HTTPException(status_code=500, detail="Server not configured for status SHORTLISTED")

    # Update by resume_ids
    if ids:
        try:
            resp = (
                supabase.table("resumes")
                .update({"status": new_status})
                .in_("resume_id", ids)
                .eq("job_id", job_id)
                .eq("status", payload.only_if_status)
                .execute()
            )
            updated_total += len(resp.data or [])
        except Exception as e:
            errors.append({"by": "resume_ids", "error": str(e)})

    # Update by emails
    if emails:
        try:
            resp = (
                supabase.table("resumes")
                .update({"status": new_status})
                .in_("email", emails)
                .eq("job_id", job_id)
                .eq("status", payload.only_if_status)
                .execute()
            )
            updated_total += len(resp.data or [])
        except Exception as e:
            errors.append({"by": "emails", "error": str(e)})

    # For visibility, report how many in the selection are currently NOT in only_if_status
    # (best-effort estimate)
    try:
        selected_count = 0
        if ids:
            selected_count += (
                supabase.table("resumes")
                .select("resume_id", count="exact")
                .in_("resume_id", ids).eq("job_id", job_id)
                .execute().count or 0
            )
        if emails:
            selected_count += (
                supabase.table("resumes")
                .select("email", count="exact")
                .in_("email", emails).eq("job_id", job_id)
                .execute().count or 0
            )
        skipped = max(0, selected_count - updated_total)
    except Exception:
        # ignore count errors, still return updated_total
        pass

    return {
        "job_id": job_id,
        "new_status": new_status,
        "updated": updated_total,
        "skipped_non_matching_status_or_missing": skipped,
        "errors": errors,
    }

from postgrest.exceptions import APIError as PgAPIError

@app.post("/interviews/schedule")
async def schedule_interview(
    body: ScheduleInterviewIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    # Verify recruiter owns the job
    _ = _get_job_owned(body.job_id, current_user)

    # ---------- candidate (resume) ----------
    res_row = _first_row(
        supabase.table("resumes")
        .select("email, full_name")
        .eq("resume_id", body.resume_id)
        .eq("job_id", body.job_id)
        .limit(1)
        .execute()
    )
    if not res_row:
        raise HTTPException(404, "Resume not found for job")
    candidate_email = res_row["email"]

    # ---------- interviewer ----------
    intv = _first_row(
        supabase.table("interviewers")
        .select("email, name")
        .eq("interviewer_id", body.interviewer_id)
        .limit(1)
        .execute()
    )
    if not intv:
        raise HTTPException(404, "Interviewer not found")
    interviewer_email = intv["email"]

    # ---------- job (and optional company) ----------
    job = _first_row(
        supabase.table("jobs")
        .select("role, company_id")
        .eq("job_id", body.job_id)
        .limit(1)
        .execute()
    )
    if not job:
        raise HTTPException(404, "Job not found")

    comp = None
    if job.get("company_id"):
        comp = _first_row(
            supabase.table("companies")
            .select("company_name")
            .eq("company_id", job["company_id"])
            .limit(1)
            .execute()
        )

    role = job.get("role") or "Interview"
    company_name = (comp or {}).get("company_name") or ""
    # ✅ this was missing before; you referenced `title` without defining it
    title = f"{role} Interview" + (f" — {company_name}" if company_name else "")

    # ---------- idempotency check ----------
    if body.external_id:
        existing = _first_row(
            supabase.table("interviews")
            .select("*")
            .eq("external_id", body.external_id)
            .limit(1)
            .execute()
        )
        if existing and existing.get("google_event_id"):
            return {
                "status": existing["status"],
                "calendarId": existing.get("google_html_link"),
                "meetLink": existing.get("google_meet_link"),
                "interview_id": existing["interview_id"],
            }

    # ---------- Google Calendar ----------
    svc = _google_service_for_user(current_user.user_id)

    # Best-effort free/busy check (skip on error)
    try:
        fb_req = {
            "timeMin": body.start_iso,
            "timeMax": body.end_iso,
            "timeZone": body.timezone,
            "items": [{"id": interviewer_email}, {"id": candidate_email}],
        }
        fb = svc.freebusy().query(body=fb_req).execute()
        busy = []
        for cal in fb.get("calendars", {}).values():
            busy.extend(cal.get("busy", []))
        if busy:
            return {"status": "CONFLICT", "busy": busy}
    except Exception:
        pass

    description = (
        f"Interview for {role}"
        + (f" at {company_name}" if company_name else "")
        + ".\n\nPlease join using the Google Meet link. "
          "If you need to reschedule, reply to this email.\n\n"
          "Agenda: 5 min intro, 35 min technical Q&A, 10 min wrap-up.\n"
          "Kindly accept or decline from the calendar invite."
    )

    event = {
        "summary": title,
        "description": description,
        "start": {"dateTime": body.start_iso, "timeZone": body.timezone},
        "end":   {"dateTime": body.end_iso,   "timeZone": body.timezone},
        "attendees": [
            {"email": candidate_email},
            {"email": interviewer_email},
            {"email": current_user.email},  # recruiter/admin
        ],
        "extendedProperties": {"shared": {"externalId": body.external_id or secrets.token_hex(8)}},
        "conferenceData": {
            "createRequest": {
                "requestId": secrets.token_hex(8),
                "conferenceSolutionKey": {"type": "hangoutsMeet"},
            }
        },
    }

    created = svc.events().insert(
        calendarId="primary",
        body=event,
        conferenceDataVersion=1,
        sendUpdates="all"
    ).execute()

    meet = created.get("hangoutLink") or (
        (created.get("conferenceData", {}).get("entryPoints") or [{}])[0].get("uri")
    )

    # ---------- persist interview ----------
    row = (
        supabase.table("interviews")
        .insert({
            "job_id": body.job_id,
            "resume_id": body.resume_id,
            "interviewer_id": body.interviewer_id,
            "candidate_email": candidate_email,
            "interviewer_email": interviewer_email,
            "recruiter_email": current_user.email,
            "status": "INTERVIEW_SCHEDULED",
            "google_event_id": created["id"],
            "google_html_link": created.get("htmlLink"),
            "google_meet_link": meet,
            "external_id": (body.external_id or event["extendedProperties"]["shared"]["externalId"]),
            "start_at": body.start_iso,
            "end_at": body.end_iso,
            "created_by": current_user.user_id,
        })
        .execute()
        .data[0]
    )

    # NEW: update resume status
    try:
        supabase.table("resumes").update(
            {"status": "INTERVIEW_SCHEDULED"}
        ).eq("resume_id", body.resume_id).execute()
    except Exception as e:
        print("Non-fatal: failed to update resume status:", e)

    return {
        "interview_id": row["interview_id"],
        "status": row["status"],
        "calendarId": row["google_html_link"],
        "meetLink": row["google_meet_link"],
        "flags": {
            "scheduled":      row["status"] == "INTERVIEW_SCHEDULED",
            "interview_done": row["status"] == "INTERVIEW_DONE",
            "select":         row["status"] == "SELECTED",
            "reject":         row["status"] == "REJECTED",
            "review":         row["status"] == "REVIEW",
            "hired":          row["status"] == "HIRED",
            "absent":         row["status"] == "ABSENT",
        },

        "attendees": created.get("attendees", []),
    }

class InterviewStatusIn(BaseModel):
    status: str  # validate against the enum in code too

@app.post("/interviews/{interview_id}/status")
async def update_interview_status(
    interview_id: str,
    body: InterviewStatusIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    allowed = {
        "INTERVIEW_SCHEDULED",
        "INTERVIEW_DONE",
        "SELECTED",
        "REJECTED",
        "REVIEW",
        "HIRED",
        "ABSENT",
        "CANCELLED",
    }
    if body.status not in allowed:
        raise HTTPException(400, "Invalid status")

    resp = (
        supabase.table("interviews")
        .update({"status": body.status})
        .eq("interview_id", interview_id)
        .execute()
    )
    rows = resp.data or []
    if not rows:
        raise HTTPException(404, "Interview not found")

    updated_row = rows[0]

    # 🔥 Sync resume status for pipeline-relevant states
    # (You can adjust this set if you don't want all to show up on resumes)
    if body.status in {
        "INTERVIEW_SCHEDULED",
        "INTERVIEW_DONE",
        "REVIEW",
        "SELECTED",
        "REJECTED",
        "HIRED",
        "ABSENT",
        "CANCELLED",
    }:
        resume_id = updated_row.get("resume_id")
        _update_resume_status(resume_id, body.status)

    return {"interview_id": interview_id, "status": body.status}

@app.post("/recruiter/interviews/{interview_id}/decision")
async def recruiter_set_decision(
    interview_id: str,
    body: RecruiterDecisionIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Recruiter sets final/next-step decision for an interview.
    Allowed statuses: REVIEW, HIRED, REJECTED, SELECTED.
    Also stores recruiter_note.
    """
    # 1) Fetch interview
    interview = _first_row(
        supabase.table("interviews")
        .select("interview_id, job_id, resume_id, status, recruiter_note")  # 👈 added resume_id
        .eq("interview_id", interview_id)
        .limit(1)
        .execute()
    )
    if not interview:
        raise HTTPException(404, "Interview not found")

    # 2) Ensure this recruiter owns the job
    _ = _get_job_owned(interview["job_id"], current_user)

    # 3) Update interview status + note
    update_data: Dict[str, Any] = {"status": body.status}
    if body.note is not None:
        update_data["recruiter_note"] = body.note

    updated = (
        supabase.table("interviews")
        .update(update_data)
        .eq("interview_id", interview_id)
        .execute()
        .data
    )
    if not updated:
        raise HTTPException(404, "Interview not found after update")

    # 4) 🔥 KEEP RESUME IN SYNC
    resume_id = interview.get("resume_id")
    _update_resume_status(resume_id, body.status)

    return {
        "interview_id": interview_id,
        "status": body.status,
        "recruiter_note": body.note,
    }

# ─────────────────────────────────────────────
# MeetAI session endpoints (reuse same DB)
# ─────────────────────────────────────────────

@app.post("/session/start")
async def meet_session_start(payload: dict):
    meeting_id = payload.get("meeting_id")
    if not meeting_id:
        raise HTTPException(400, "meeting_id required")
    if not SUPABASE_ENABLED:
        raise HTTPException(
            500,
            "Supabase not configured on server (.env missing SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY)",
        )

    row: dict = {"meeting_id": meeting_id}
    for k in ("jd", "resume", "interviewer_name", "candidate_name"):
        v = payload.get(k)
        if v is not None:
            row[k] = v

    supabase.table("meetings").upsert(row, on_conflict="meeting_id").execute()
    return {"session_id": meeting_id}

# ADD NEAR OTHER IMPORTS
from fastapi import Body

@app.post("/meet/session/bootstrap")
async def meet_session_bootstrap(
    payload: Dict[str, str] = Body(...),
    current_user: UserIdentity = Depends(get_current_user),
):
    """
    Create / upsert a MeetAI 'session' (meetings row) using ONLY interview_id.
    This means the interviewer never has to type JD, resume text, candidate name, etc.
    
    Frontend will:
      1) Call this endpoint with { "interview_id": "..." }
      2) Receive: { session_id, meeting_id, candidate_name, interviewer_name, jd, resume }
      3) Use session_id/meeting_id when connecting to MeetAI WebSocket or /ai endpoints.
    """
    interview_id = (payload or {}).get("interview_id")
    if not interview_id:
        raise HTTPException(400, "interview_id required")

    # Use your existing helper: verifies interviewer owns this interview
    ctx = _get_interview_context(interview_id, current_user)
    job = ctx["job"]
    resume = ctx["resume"]
    interviewer = ctx["interviewer"]

    # Build the values MeetAI expects
    meeting_id = interview_id  # <-- use interview_id as session/meeting id
    jd_text = job.get("jd_text") or ""
    resume_text = resume.get("raw_text") or ""
    candidate_name = (
        resume.get("full_name")
        or resume.get("candidate_name")
        or resume.get("email")
        or "Candidate"
    )
    interviewer_name = interviewer.get("name") or interviewer.get("email") or "Interviewer"

    # We are reusing the same Supabase client as everywhere else
    # Upsert into 'meetings' table used by MeetAI:
    row = {
        "meeting_id": meeting_id,
        "jd": jd_text,
        "resume": resume_text,
        "interviewer_name": interviewer_name,
        "candidate_name": candidate_name,
    }

    # NOTE: uses same table/shape as meetai.py's start_session()
    supabase.table("meetings").upsert(row, on_conflict="meeting_id").execute()

    return {
        "session_id": meeting_id,
        "meeting_id": meeting_id,
        "candidate_name": candidate_name,
        "interviewer_name": interviewer_name,
        "jd": jd_text,
        "resume": resume_text,
    }

@app.get("/session/{session_id}/turns")
async def meet_get_turns(session_id: str):
    mtg = _sb_get_meeting(session_id)
    pseudo = _parse_transcript_to_turns(
        mtg.get("transcript") or "",
        mtg.get("interviewer_name") or "",
        mtg.get("candidate_name") or "",
    )
    return {"session_id": session_id, "turns": pseudo}

# ─────────────────────────────────────────────
# WebSocket STT
# ─────────────────────────────────────────────
@app.websocket("/ws/{source}")
async def websocket_stt(websocket: WebSocket, source: str):
    if source not in ("mic", "tab"):
        await websocket.close(code=1008)
        return
    await websocket.accept()
    qp = websocket.query_params
    session_id = qp.get("session_id")
    speaker = qp.get("speaker") or ("interviewer" if source == "mic" else "candidate")
    speaker_name = qp.get("speaker_name") or speaker

    print(
        f"[ws:{source}] connected (session={session_id}, speaker={speaker}, name={speaker_name})"
    )

    try:
        stt_client = get_speech_client()
    except Exception as e:
        print(f"[stt-init] error: {e}")
        await websocket.close()
        return

    audio_q: "queue.Queue[Optional[bytes]]" = queue.Queue()
    out_q: "asyncio.Queue[Optional[dict]]" = asyncio.Queue()

    worker = threading.Thread(
        target=google_stt_worker,
        args=(
            source,
            session_id,
            speaker,
            speaker_name,
            stt_client,
            audio_q,
            asyncio.get_running_loop(),
            out_q,
        ),
        daemon=True,
    )
    worker.start()

    sender_task = asyncio.create_task(result_sender(websocket, out_q, source))

    try:
        while True:
            try:
                msg = await websocket.receive()
            except WebSocketDisconnect:
                print(f"[ws:{source}] disconnected")
                break
            except RuntimeError as e:
                if "disconnect message has been received" in str(e).lower():
                    print(f"[ws:{source}] disconnect acknowledged")
                    break
                raise
            if msg.get("bytes") is not None:
                audio_q.put(msg["bytes"])  # PCM16 bytes
    finally:
        audio_q.put(None)
        try:
            await asyncio.wait_for(sender_task, timeout=2)
        except asyncio.TimeoutError:
            sender_task.cancel()
        print(f"[ws:{source}] done")

@app.post("/ai/summary")
async def ai_summary(payload: Dict[str, str] = Body(...)):
    session_id = (payload or {}).get("session_id")
    if not session_id:
        raise HTTPException(400, "session_id required")

    meeting = _sb_get_meeting(session_id)
    turns = _recent_turns(session_id, limit=2000)
    resume = meeting.get("resume", "")
    jd = meeting.get("jd", "")

    lines = [
        f"{(t.get('speaker_name') or t.get('speaker') or t.get('source'))}: {t.get('text','')}"
        for t in turns
    ]
    convo = "\n".join(lines)

    prompt = f"""
You are helping a recruiter. Return STRICT JSON ONLY with exactly these keys:

analytics: object with:
  - skills_match: integer 0–10
      How well the candidate's REAL skills (from RESUME and what they actually said in the interview(according to the transcript)) match the JD.
      Do NOT invent skills or experience that are not supported by the resume or transcript.
  - communication_experience: integer 0–10
      How clearly the candidate communicates according to the questions asked by the interviewer.
      If the candidate ahs not responded to the question or no proper conversation went then score 1 .
  - jd_alignment: integer 0–10
      How well the RESUME + spoken answers align with the responsibilities and requirements in the JD.
  - overall_score: integer 0–10
      Your estimate of overall fit. (The server will recompute this as the average of the first three scores.)

role_fit: one of
  "perfect fit", "good fit", "partial fit", "uncertain", "not a fit"

experience:
  A short label such as "fresher", "mid level", or "senior", based mainly on RESUME (years/roles) and then interview.

strengths:(write in bullet points)
  A short, neutral 5-6 line paragraph describing the key strengths shown in the RESUME and INTERVIEW.
  Focus on real evidence from their background and what they actually said.

weaknesses:(write in bullet points)
  A short, constructive 5–6 line paragraph about growth areas.
  Never be rude or harsh. If information is limited, say that evaluation is limited instead of criticizing.
  If the candidate does not gave any answer or has not responded to the question then mention it. 

interview_summary:(write in bullet points)
  A short, neutral 5–6 line paragraph summarizing how the interview went:
  level of detail in answers, confidence vs confusion, and overall impression.
  Do NOT assume things that were never said.

IMPORTANT RULES:
- Use RESUME and FULL TRANSCRIPT as the single source of truth about the candidate.
- Use the JD only to judge relevance and alignment, NOT to fabricate missing skills.
- If the candidate spoke very little or gave almost no answers (e.g. mostly interviewer talking, or very short replies),
  then keep all analytics scores low (e.g. between 1 and 3) and clearly mention that the assessment is limited by lack of data.
- Do not over-penalize nervousness or minor pauses; focus on the quality and clarity of whatever the candidate actually explained.
- Be professional, supportive, and non-decisive. The recruiter makes the final decision, you are only assisting.
- Output JSON only. Do not include any extra text or explanations.
- If there is very limited conversation happened like (hi , hello , am i audible) , then do write that in the Interview summary and weaknessess.
- Do not give "\\n" in responses.

RESUME:
{resume}

JD:
{jd}

FULL TRANSCRIPT:
{convo}
""".strip()

    data = _ollama_chat_json(prompt)
    coerced = _coerce_summary_payload(data)

    try:
        _sb_upsert_ai_report(session_id, coerced)
    except Exception as e:
        print(f"[ai_summary] store report error: {e}")

    return {"session_id": session_id, **coerced}

@app.post("/ai/expected")
async def ai_expected(payload: Dict[str, str] = Body(...)):
    session_id = (payload or {}).get("session_id")
    question = (payload or {}).get("question", "").strip()
    if not session_id or not question:
        raise HTTPException(400, "session_id and question required")

    meeting = _sb_get_meeting(session_id)
    resume = meeting.get("resume", "")
    jd = meeting.get("jd", "")
    turns = _recent_turns(session_id, 80)
    convo = _turns_snippet(turns, 12)

    prompt = f"""
You are generating an example answer to help THIS candidate improve.
Return STRICT JSON with exactly one key: expected_answer.

Rules:
- Use RESUME and RECENT TRANSCRIPT as the SINGLE source of truth about the candidate's background and skills.
- Do NOT invent technical skills, tools, projects, or years of experience that are not clearly present in the RESUME or mentioned by the candidate.
- Use the JD only to decide what to emphasize and how to structure the answer, but keep the content honest and realistic for THIS candidate.
- If the JD asks for skills that the candidate does not have, you may:
    • Mention genuine interest in learning those areas, OR
    • Highlight transferable skills from their real experience,
  but you MUST NOT pretend they already have that experience.
- The answer should work for both technical and non-technical candidates:
    use simple, clear language, and avoid heavy jargon.
- Be supportive and neutral; do not sound judgmental.
- always give complete full answers, DO NOT PROVIDE INCOMPLETE AND HALF ANSWERS.
- write in point wise answers.

Context:
RESUME:
{resume}

JD:
{jd}

RECENT TRANSCRIPT:
{convo}

QUESTION TO PREPARE EXPECTED ANSWER FOR:
{question}

Write a concise but complete expected_answer (bullets or short paragraphs)
that THIS candidate could realistically say based on their real background.
""".strip()

    data = _ollama_chat_json(prompt)
    exp_raw = (data.get("expected_answer") or data.get("raw") or "").strip()
    return {
        "session_id": session_id,
        "question": question,
        "expected_answer": _to_bullets(exp_raw, max_items=6),
    }

@app.post("/ai/validate")
async def ai_validate(payload: Dict[str, str] = Body(...)):
    session_id = (payload or {}).get("session_id")
    question = (payload or {}).get("question", "").strip()
    if not session_id or not question:
        raise HTTPException(400, "session_id and question required")

    meeting = _sb_get_meeting(session_id)
    resume = meeting.get("resume", "")
    jd = meeting.get("jd", "")
    turns = _recent_turns(session_id, 300)

    cand_answer = _extract_candidate_answer(
        question,
        turns,
        interviewer_name=(meeting.get("interviewer_name") or ""),
        candidate_name=(meeting.get("candidate_name") or ""),
    )

    lines = [
        f"{(t.get('speaker_name') or t.get('speaker') or t.get('source'))}: {t.get('text', '')}"
        for t in turns
    ]
    full_convo = "\n".join(lines)

    prompt = f"""
You are evaluating a candidate's answer to an interview question.
Return STRICT JSON with keys:
- verdict (one of: "STRONG", "OK", "LIMITED")
- score (0.0–1.0)
- explanation (1–3 short sentences)
- expected_answer
- candidate_answer

Use:
RESUME:
{resume}

JD:
{jd}

QUESTION:
{question}

FULL_TRANSCRIPT:
{full_convo}

CANDIDATE_ANSWER (already extracted by the system from the transcript):
{cand_answer if cand_answer else "(none)"}

GUIDELINES:

1) EXPECTED ANSWER:
   - Base the expected_answer primarily on the QUESTION itself.
   - Use JD + RESUME only to adjust which topics or technologies to emphasize,
     but do NOT require the expected answer to mention specific projects, metrics,
     or every single tool from the resume unless the question explicitly asks for that.
   - The expected_answer should be realistic for THIS candidate's background.
   - Write a complete response. Do NOT write an incomplete sentence.

2) VERDICT:
   - "STRONG": Candidate answer is largely correct, relevant, and reasonably complete
     for the QUESTION asked.
   - "OK": Candidate answer is partially correct or high-level, but missing some important
     details OR not very well structured.
   - "LIMITED": Candidate answer is very short, off-topic, clearly incorrect,
     or contradicts the resume/JD.

   SPECIAL RULE FOR SIMPLE / DEFINITION QUESTIONS:
   - For short conceptual/definition questions like "What is React?",
     if the candidate gives a correct definition with the main idea and 1–2 relevant details,
     treat this as STRONG, even if they don't mention JSX, hooks, lifecycle, or performance
     optimizations, unless the question explicitly asked for a "detailed" or "in-depth" answer.

   VERY IMPORTANT:
   - If CANDIDATE_ANSWER is NOT empty, you MUST NOT say that the candidate "did not provide any information"
     or "gave no answer". You must at least acknowledge what they DID say, even if it was brief.
   - For broad questions like "Tell me about yourself" / "Introduce yourself" /
     "Walk me through your profile":
     • If the answer is generally consistent with the resume and JD (even if brief),
       use at least "OK", not "LIMITED".
     • Do NOT mark an answer as bad just because it doesn't repeat every project or metric
       from the resume.
     • Minor differences in phrasing or missing achievements are acceptable.

3) SCORE:
   - score is a float between 0.0 and 1.0.
   - Rough guideline:
     • STRONG  → score between 0.7 and 1.0
     • OK      → score between 0.4 and 0.7
     • LIMITED → score between 0.0 and 0.4

4) TONE:
   - Be neutral, kind, and constructive.
   - In the explanation, briefly mention what was good and one area to improve.
   - Do NOT list a long laundry list of missing topics.
   - Avoid harsh language like "wrong", "terrible", or personal judgments.

Return ONLY the JSON object, with no extra commentary.
""".strip()

    data = _ollama_chat_json(prompt)

    # Softer verdict labels
    verdict = (data.get("verdict") or "").upper().strip()
    allowed_verdicts = {"STRONG", "OK", "LIMITED"}
    if verdict not in allowed_verdicts:
        # Fallback: if there is no candidate answer at all, LIMITED; otherwise OK
        verdict = "LIMITED" if not cand_answer else "OK"

    try:
        score = float(data.get("score", 0.0))
    except Exception:
        score = 0.0
    # Clamp score into [0.0, 1.0]
    score = max(0.0, min(1.0, score))
    score_pct = int(round(score * 100))

    exp_raw = (data.get("expected_answer") or data.get("raw") or "").strip()
    explanation_raw = (data.get("explanation") or data.get("raw") or "").strip()

    return {
        "session_id": session_id,
        "question": question,
        "expected_answer": _to_bullets(exp_raw, max_items=6,max_len=220),
        "candidate_answer": cand_answer,
        "verdict": verdict,          # STRONG / OK / LIMITED
        "score": score,
        "score_pct": score_pct,
        "explanation": _to_bullets(explanation_raw, max_items=3, max_len=220),
    }

@app.post("/ai/questions")
async def ai_questions(payload: Dict[str, Any] = Body(...)):
    session_id = (payload or {}).get("session_id")
    count = int((payload or {}).get("count", 3))
    if not session_id:
        raise HTTPException(400, "session_id required")

    meeting = _sb_get_meeting(session_id)
    resume = meeting.get("resume", "")
    jd = meeting.get("jd", "")
    turns = _recent_turns(session_id, 60)
    convo = _turns_snippet(turns, 10)

    prompt = f"""
You are helping a recruiter prepare follow-up questions AND example answers
for THIS candidate.

Return STRICT JSON with exactly one key: "items".

"items" MUST be an array of objects. Each object MUST have:
  - "question": string  (concise follow-up interview question)
  - "answer":   string  (short 2 line example answer this candidate could realistically give)

You MUST return up to 3 items (fewer only if it truly doesn't make sense).

Rules for answers:
- Use RESUME and RECENT TRANSCRIPT as the SINGLE source of truth about the candidate's background.
- Do NOT invent tools, technologies, projects, or years of experience that contradict the RESUME.
- If there is very little information about the candidate:
    • Still write a helpful, realistic 2 line example answer
      (for example, from the perspective of a fresher or early-career candidate),
      instead of saying you cannot answer.
- Use simple, clear language (no heavy jargon).
- Be neutral, supportive, and non-judgmental.
- Do NOT leave "answer" empty or null for any item.

Context:
RESUME:
{resume}

JD:
{jd}

RECENT TRANSCRIPT:
{convo if convo.strip() else "(none)"}

Return ONLY JSON, no code fences, no markdown, no extra text.
Expected shape:
{{ "items": [{{"question":"...","answer":"..."}}, ...] }}
""".strip()

    data = _ollama_chat_json(prompt, temperature=0.5)

    items = data.get("items") or []

    questions: List[str] = []
    answers: List[str] = []

    if isinstance(items, list):
        for item in items:
            if not isinstance(item, dict):
                continue
            q = str(item.get("question") or "").strip()
            a_raw = str(
                item.get("answer")
                or item.get("expected_answer")
                or ""
            ).strip()

            if not q:
                continue

            # format the answer nicely into bullets, but don't let it become empty
            formatted = _to_bullets(a_raw, max_items=6)
            if not formatted:
                formatted = a_raw or "Example answer could highlight one concrete experience and connect it to this role."

            questions.append(q)
            answers.append(formatted)

    # Hard fallback: if the model didn't follow the spec at all
    if not questions:
        raw = (data.get("raw") or "").strip()
        if raw:
            # naive fallback: split lines as questions, generic placeholder answers
            lines = [x.strip("-• ").strip() for x in raw.split("\n") if x.strip()]
            for line in lines[:count]:
                questions.append(line)
                answers.append("You can answer this by briefly describing a relevant situation, what you did, and the outcome.")

    # Trim to requested count, keep arrays aligned
    n = min(len(questions), len(answers), count)
    questions = questions[:n]
    answers = answers[:n]

    return {
        "session_id": session_id,
        "questions": questions,
        "answers": answers,
    }

@app.post("/recruiter/interviews/{interview_id}/email/hired")
async def send_hired_email(
    interview_id: str,
    body: HiredEmailIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    ctx = _get_recruiter_interview_context(interview_id, current_user)
    interview, job, resume, company = ctx["interview"], ctx["job"], ctx["resume"], ctx["company"]

    # Idempotency: if already sent, just return
    if interview.get("hired_email_sent_at"):
        return {
            "interview_id": interview_id,
            "already_sent": True,
        }

    candidate_email = (resume or {}).get("email") or interview.get("candidate_email")
    candidate_name = (resume or {}).get("full_name")
    role = job.get("role") or "Position"
    company_name = (company or {}).get("company_name") or "our company"

    description = _hired_email_body(
        candidate_name=candidate_name,
        role=role,
        company_name=company_name,
        start_iso=body.start_iso,
        location=body.location,
    )

    try:
        svc = _google_service_for_user(current_user.user_id)
    except HTTPException:
        # bubble up our own HTTPException
        raise
    except Exception as e:
        raise HTTPException(500, f"Google connection failed: {e}")

    event = {
        "summary": f"Offer Confirmation — {role} at {company_name}",
        "location": body.location,
        "start": {"dateTime": body.start_iso, "timeZone": body.timezone},
        "end":   {"dateTime": body.end_iso,   "timeZone": body.timezone},
        "attendees": [
            {"email": candidate_email},
            {"email": current_user.email},
        ],
        "description": description,
    }

    try:
        created = svc.events().insert(
            calendarId="primary",
            body=event,
            sendUpdates="all",
        ).execute()
    except Exception as e:
        raise HTTPException(502, f"Failed to create calendar event: {e}")

    # Mark as sent + optionally bump status to HIRED
    now_iso = datetime.datetime.utcnow().isoformat() + "Z"
    supabase.table("interviews").update(
        {
            "hired_email_sent_at": now_iso,
            "status": "HIRED",
        }
    ).eq("interview_id", interview_id).execute()

    return {
        "interview_id": interview_id,
        "status": "HIRED",
        "already_sent": False,
        "calendar_link": created.get("htmlLink"),
    }

@app.post("/recruiter/interviews/{interview_id}/email/rejected")
async def send_rejected_email(
    interview_id: str,
    body: RejectEmailIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    ctx = _get_recruiter_interview_context(interview_id, current_user)
    interview, job, resume, company = ctx["interview"], ctx["job"], ctx["resume"], ctx["company"]

    if interview.get("rejected_email_sent_at"):
        return {
            "interview_id": interview_id,
            "already_sent": True,
        }

    candidate_email = (resume or {}).get("email") or interview.get("candidate_email")
    candidate_name = (resume or {}).get("full_name")
    role = job.get("role") or "Position"
    company_name = (company or {}).get("company_name") or "our company"

    desc = _rejected_email_body(
        candidate_name=candidate_name,
        role=role,
        company_name=company_name,
    )

    # All-day event date
    if body.date:
        date_str = body.date
    else:
        date_str = datetime.date.today().isoformat()

    try:
        svc = _google_service_for_user(current_user.user_id)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"Google connection failed: {e}")

    # All-day event uses 'date' instead of 'dateTime'
    event = {
        "summary": f"Application decision — {role} at {company_name}",
        "start": {"date": date_str},
        "end": {"date": date_str},
        "attendees": [
            {"email": candidate_email},
            {"email": current_user.email},
        ],
        "description": desc,
    }

    try:
        created = svc.events().insert(
            calendarId="primary",
            body=event,
            sendUpdates="all",
        ).execute()
    except Exception as e:
        raise HTTPException(502, f"Failed to create calendar event: {e}")

    now_iso = datetime.datetime.utcnow().isoformat() + "Z"
    supabase.table("interviews").update(
        {
            "rejected_email_sent_at": now_iso,
            "status": "REJECTED",
        }
    ).eq("interview_id", interview_id).execute()

    return {
        "interview_id": interview_id,
        "status": "REJECTED",
        "already_sent": False,
        "calendar_link": created.get("htmlLink"),
    }

@app.post("/recruiter/interviews/{interview_id}/email/assignment")
async def send_assignment_email(
    interview_id: str,
    body: AssignmentEmailIn,
    current_user: UserIdentity = Depends(get_current_user),
):
    ctx = _get_recruiter_interview_context(interview_id, current_user)
    interview, job, resume, company = ctx["interview"], ctx["job"], ctx["resume"], ctx["company"]

    if interview.get("assignment_email_sent_at"):
        return {
            "interview_id": interview_id,
            "already_sent": True,
        }

    candidate_email = (resume or {}).get("email") or interview.get("candidate_email")
    candidate_name = (resume or {}).get("full_name")
    role = job.get("role") or "Position"
    company_name = (company or {}).get("company_name") or "our company"

    desc = _assignment_email_body(
        candidate_name=candidate_name,
        role=role,
        company_name=company_name,
        deadline_iso=body.deadline_iso,
        link=body.link,
        description=body.description,
    )

    try:
        svc = _google_service_for_user(current_user.user_id)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"Google connection failed: {e}")

    # Make event 30 minutes long ending at deadline
    try:
        dt_deadline = datetime.datetime.fromisoformat(body.deadline_iso.replace("Z", "+00:00"))
    except Exception:
        raise HTTPException(400, "Invalid deadline_iso format")

    end_dt = dt_deadline + datetime.timedelta(minutes=30)
    end_iso = end_dt.isoformat()

    event = {
        "summary": f"Assignment for {role} — {company_name}",
        "start": {"dateTime": body.deadline_iso, "timeZone": body.timezone},
        "end":   {"dateTime": end_iso,          "timeZone": body.timezone},
        "attendees": [
            {"email": candidate_email},
            {"email": current_user.email},
        ],
        "description": desc,
    }

    try:
        created = svc.events().insert(
            calendarId="primary",
            body=event,
            sendUpdates="all",
        ).execute()
    except Exception as e:
        raise HTTPException(502, f"Failed to create calendar event: {e}")

    now_iso = datetime.datetime.utcnow().isoformat() + "Z"
    supabase.table("interviews").update(
        {
            "assignment_email_sent_at": now_iso,
        }
    ).eq("interview_id", interview_id).execute()

    return {
        "interview_id": interview_id,
        "already_sent": False,
        "calendar_link": created.get("htmlLink"),
    }

# ---------------------------------------------------------------------------
# run (local)
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
